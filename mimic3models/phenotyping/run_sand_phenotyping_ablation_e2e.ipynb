{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ce6152-2484-4dd3-b29f-d5f79dacc3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current File Path: /home/jovyan/mimic3-sand/mimic3models/phenotyping\n",
      "Base Path: /home/jovyan/mimic3-sand\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import tqdm\n",
    "import wandb\n",
    "\n",
    "cur_path = Path(\".\").resolve()\n",
    "base_path = cur_path.parents[1]\n",
    "\n",
    "print(f\"Current File Path: {cur_path}\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "\n",
    "os.chdir(str(base_path))\n",
    "sys.path.append(str(base_path))\n",
    "\n",
    "# Local modules\n",
    "from mimic3benchmark.readers import PhenotypingReader\n",
    "from mimic3models import common_utils\n",
    "from mimic3models.phenotyping import utils\n",
    "from mimic3models.preprocessing import Discretizer, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbedd523-9d91-4f4c-9eb9-7887e4975765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11f70a6-cb53-422e-b5c4-ff98c9479c2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5869a166-88e9-4b03-b20f-76a005a50bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, seq_len = 10000) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "\n",
    "        for pos in range(seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
    "                pe[pos, i+1] = math.cos(pos / (10000 ** ((2 * (i+1)) / d_model)))\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        seq_len = x.shape[1]\n",
    "        x = math.sqrt(self.d_model) * x\n",
    "        x = x + self.pe[:, :seq_len].requires_grad_(False)\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, layer: nn.Module, embed_dim: int, p=0.1) -> None:\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.layer = layer\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.attn_weights = None\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, pad_mask = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param x: [N, seq_len, features]\n",
    "        :return: [N, seq_len, features]\n",
    "        \"\"\"\n",
    "        \n",
    "        if isinstance(self.layer, nn.MultiheadAttention):\n",
    "            BS, seq_len, _ = x.shape\n",
    "            src = x.transpose(0, 1)     # [seq_len, N, features]\n",
    "            # mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(torch.bool).to(device)\n",
    "            output, self.attn_weights = self.layer(src, src, src, key_padding_mask=pad_mask, attn_mask=None)\n",
    "            output = output.transpose(0, 1)     # [N, seq_len, features]\n",
    "        else:\n",
    "            output = self.layer(x)\n",
    "\n",
    "        output = self.dropout(output)\n",
    "        output = self.norm(x + output)\n",
    "        return output\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size: int) -> None:\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(hidden_size, hidden_size * 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_size * 2, hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        tensor = tensor.transpose(1, 2)\n",
    "        tensor = self.conv(tensor)\n",
    "        tensor = tensor.transpose(1, 2)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_head: int, dropout_rate=0.1) -> None:\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attention = ResidualBlock(\n",
    "            nn.MultiheadAttention(embed_dim, num_head), embed_dim, p=dropout_rate\n",
    "        )\n",
    "        self.ffn = ResidualBlock(PositionWiseFeedForward(embed_dim), embed_dim, p=dropout_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, pad_mask) -> torch.Tensor:\n",
    "        x = self.attention(x, pad_mask)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "class ClassificationModule(nn.Module):\n",
    "    def __init__(self, d_model: int, num_class: int) -> None:\n",
    "        super(ClassificationModule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_class = num_class\n",
    "\n",
    "        self.fc = nn.Linear(d_model, num_class)\n",
    "\n",
    "        nn.init.normal_(self.fc.weight, std=0.02)\n",
    "        nn.init.normal_(self.fc.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2140e681-eafc-4027-87d4-56f21082ef41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLayerForSAnD(nn.Module):\n",
    "    def __init__(self, input_features, n_heads, n_layers, d_model=128, dropout_rate=0.2) -> None:\n",
    "        super(EncoderLayerForSAnD, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.input_embedding = nn.Conv1d(input_features, d_model, 1)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            EncoderBlock(d_model, n_heads, dropout_rate) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, pad_mask) -> torch.Tensor:\n",
    "        BS, seq_len, emb_dim = x.shape\n",
    "        \n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.input_embedding(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        cls = self.cls_token.expand(BS, -1, -1).to(device)  \n",
    "        x = torch.cat([cls, x], dim=1)\n",
    "        \n",
    "        pad_mask = torch.cat(\n",
    "            [torch.zeros((BS, 1), dtype=torch.bool, device=device), pad_mask], \n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        for l in self.blocks:\n",
    "            x = l(x, pad_mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SAnD(nn.Module):\n",
    "    \"\"\"\n",
    "    Simply Attend and Diagnose model\n",
    "\n",
    "    The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)\n",
    "\n",
    "    `Attend and Diagnose: Clinical Time Series Analysis Using Attention Models <https://arxiv.org/abs/1711.03905>`_\n",
    "    Huan Song, Deepta Rajan, Jayaraman J. Thiagarajan, Andreas Spanias\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, input_features: int, n_heads: int,\n",
    "            n_class: int, n_layers: int, d_model: int = 128, dropout_rate: float = 0.2\n",
    "    ) -> None:\n",
    "        super(SAnD, self).__init__()\n",
    "        \n",
    "        self.hyperparams = {\n",
    "             \"input_features\" : input_features,\n",
    "            \"n_heads\" : n_heads, \n",
    "            \"n_class\" : n_class,\n",
    "            \"n_layers\" : n_layers,\n",
    "            \"d_model\" : d_model,\n",
    "            \"dropout_rate\" : dropout_rate\n",
    "        }\n",
    "        \n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.encoder = EncoderLayerForSAnD(input_features, n_heads, n_layers, d_model, dropout_rate)\n",
    "\n",
    "        self.clf = ClassificationModule(d_model, n_class)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, pad_mask) -> torch.Tensor:\n",
    "        x = self.encoder(x, pad_mask)\n",
    "        \n",
    "        x = x[:, 0, :]\n",
    "        x = self.clf(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0444100e-1d2e-45ac-9bc6-b15043f067fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion=None, device=device, num_classes=25):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, pad_mask in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(inputs, pad_mask)  # shape: [batch_size, num_classes]\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            probs = F.softmax(logits, dim=1)  # probabilities for all classes\n",
    "\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "            \n",
    "            total_loss += loss.cpu().item()\n",
    "\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "\n",
    "    # Multi-class classification\n",
    "    all_labels_bin = label_binarize(all_labels, classes=list(range(num_classes)))\n",
    "    auc_micro = roc_auc_score(all_labels_bin, all_probs, average='micro', multi_class='ovr')\n",
    "    auc_macro = roc_auc_score(all_labels_bin, all_probs, average='macro', multi_class='ovr')\n",
    "    auc_weighted = roc_auc_score(all_labels_bin, all_probs, average='weighted', multi_class='ovr')\n",
    "    return {\n",
    "        \"loss\" : total_loss,\n",
    "        \"AUROC_micro\": auc_micro,\n",
    "        \"AUROC_macro\": auc_macro,\n",
    "        \"AUROC_weighted\": auc_weighted\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "887a1c9a-b565-4f26-999b-b15b44911d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, model, criterion, optimizer, optimizer_config: dict, experiment) -> None:\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = optimizer(self.model.parameters(), **optimizer_config)\n",
    "        self.criterion = criterion\n",
    "\n",
    "        self.hyper_params = optimizer_config\n",
    "        self._start_epoch = 0\n",
    "        self.hyper_params[\"epochs\"] = self._start_epoch\n",
    "        self.__num_classes = None\n",
    "        self._is_parallel = False\n",
    "        \n",
    "        self.run = wandb.init(\n",
    "            project='sand-mimic3',\n",
    "            config={\n",
    "                \"task\" : \"phenotype\",\n",
    "                \"hyperparams\" : self.model.hyperparams\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "            self._is_parallel = True\n",
    "\n",
    "            notice = \"Running on {} GPUs.\".format(torch.cuda.device_count())\n",
    "            print(\"\\033[33m\" + notice + \"\\033[0m\")\n",
    "            \n",
    "            \n",
    "\n",
    "    def fit(self, loader: Dict[str, DataLoader], epochs: int, checkpoint_path: str = None, validation: bool = True) -> None:\n",
    "        len_of_train_dataset = len(loader[\"train\"].dataset)\n",
    "        epochs = epochs + self._start_epoch\n",
    "\n",
    "        self.hyper_params[\"epochs\"] = epochs\n",
    "        self.hyper_params[\"batch_size\"] = loader[\"train\"].batch_size\n",
    "        self.hyper_params[\"train_ds_size\"] = len_of_train_dataset\n",
    "        \n",
    "        best_model = None\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "        if validation:\n",
    "            len_of_val_dataset = len(loader[\"val\"].dataset)\n",
    "            self.hyper_params[\"val_ds_size\"] = len_of_val_dataset\n",
    "\n",
    "        for epoch in range(self._start_epoch, epochs):\n",
    "            if checkpoint_path is not None and epoch % 10 == 0:\n",
    "                self.save_to_file(checkpoint_path)\n",
    "\n",
    "            correct = 0.0\n",
    "            total = 0.0\n",
    "\n",
    "            self.model.train()\n",
    "            pbar = tqdm.tqdm(total=len_of_train_dataset)\n",
    "            total_loss = 0\n",
    "            for x, y, pad_mask in loader[\"train\"]:\n",
    "                b_size = y.shape[0]\n",
    "                total += y.shape[0]\n",
    "                x = x.to(self.device) if isinstance(x, torch.Tensor) else [i.to(self.device) for i in x]\n",
    "                y = y.to(self.device)\n",
    "                pad_mask = pad_mask.to(self.device)\n",
    "\n",
    "                pbar.set_description(\n",
    "                    \"\\033[36m\" + \"Training\" + \"\\033[0m\" + \" - Epochs: {:03d}/{:03d}\".format(epoch+1, epochs)\n",
    "                )\n",
    "                pbar.update(b_size)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(x, pad_mask)\n",
    "                loss = self.criterion(outputs, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == y).sum().float().cpu().item()\n",
    "\n",
    "                total_loss += loss.cpu().item()\n",
    "\n",
    "            total_loss = total_loss / len(loader['train'])\n",
    "\n",
    "            if validation:\n",
    "                with torch.no_grad():\n",
    "                    self.model.eval()\n",
    "                    eval_result = evaluate_model(self.model, loader['val'], criterion=self.criterion)\n",
    "\n",
    "                    if eval_result['loss'] < best_val_loss:\n",
    "                        best_val_loss = eval_result['loss']\n",
    "                        state = self.model.module.state_dict() if self._is_parallel else self.model.state_dict()\n",
    "                        best_model = deepcopy(state)\n",
    "\n",
    "            if validation:\n",
    "                self.run.log({\n",
    "                    'train_loss' : total_loss, \n",
    "                    'train_accuracy' : float(correct / total), \n",
    "                    'val_loss' : eval_result['loss'], \n",
    "                    'val_AUROC_micro' : eval_result['AUROC_micro'],\n",
    "                    \"val_AUROC_macro\" : eval_result['AUROC_macro'],\n",
    "                    'val_AUROC_weighted' : eval_result['AUROC_weighted']\n",
    "                })\n",
    "            else:\n",
    "                self.run.log({\n",
    "                    'train_loss' : total_loss, \n",
    "                    'train_accuracy' : float(correct / total)\n",
    "                })\n",
    "\n",
    "            pbar.close()\n",
    "\n",
    "        if best_model is not None:\n",
    "            if self._is_parallel:\n",
    "                self.model.module.load_state_dict(best_model)\n",
    "            else:\n",
    "                self.model.load_state_dict(best_model)\n",
    "        \n",
    "        if checkpoint_path is not None:\n",
    "            self.save_to_file(checkpoint_path)\n",
    "\n",
    "    def save_checkpoint(self) -> dict:\n",
    "        \"\"\"\n",
    "        The method of saving trained PyTorch model.\n",
    "\n",
    "        Note,  return value contains\n",
    "            - the number of last epoch as `epochs`\n",
    "            - optimizer state as `optimizer_state_dict`\n",
    "            - model state as `model_state_dict`\n",
    "\n",
    "        ::\n",
    "\n",
    "            clf = NeuralNetworkClassifier(\n",
    "                    Network(), nn.CrossEntropyLoss(),\n",
    "                    optim.Adam, optimizer_config, experiment\n",
    "                )\n",
    "\n",
    "            clf.fit(train_loader, epochs=10)\n",
    "            checkpoints = clf.save_checkpoint()\n",
    "\n",
    "        :return: dict {'epoch', 'optimizer_state_dict', 'model_state_dict'}\n",
    "        \"\"\"\n",
    "\n",
    "        checkpoints = {\n",
    "            \"epoch\": deepcopy(self.hyper_params[\"epochs\"]),\n",
    "            \"optimizer_state_dict\": deepcopy(self.optimizer.state_dict())\n",
    "        }\n",
    "\n",
    "        if self._is_parallel:\n",
    "            checkpoints[\"model_state_dict\"] = deepcopy(self.model.module.state_dict())\n",
    "        else:\n",
    "            checkpoints[\"model_state_dict\"] = deepcopy(self.model.state_dict())\n",
    "\n",
    "        return checkpoints\n",
    "\n",
    "    def save_to_file(self, path: str) -> str:\n",
    "        \"\"\"\n",
    "        | The method of saving trained PyTorch model to file.\n",
    "        | Those weights are uploaded to comet.ml as backup.\n",
    "        | check \"Asserts\".\n",
    "\n",
    "        Note, .pth file contains\n",
    "            - the number of last epoch as `epochs`\n",
    "            - optimizer state as `optimizer_state_dict`\n",
    "            - model state as `model_state_dict`\n",
    "\n",
    "        ::\n",
    "\n",
    "            clf = NeuralNetworkClassifier(\n",
    "                    Network(), nn.CrossEntropyLoss(),\n",
    "                    optim.Adam, optimizer_config, experiment\n",
    "                )\n",
    "\n",
    "            clf.fit(train_loader, epochs=10)\n",
    "            filename = clf.save_to_file('path/to/save/dir/')\n",
    "\n",
    "        :param path: path to saving directory. : string\n",
    "        :return: path to file : string\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(path):\n",
    "            os.mkdir(path)\n",
    "\n",
    "        file_name = \"model_params-epochs_{}-{}.pth\".format(\n",
    "            self.hyper_params[\"epochs\"], time.ctime().replace(\" \", \"_\")\n",
    "        )\n",
    "        path = path + file_name\n",
    "\n",
    "        checkpoints = self.save_checkpoint()\n",
    "\n",
    "        torch.save(checkpoints, path)\n",
    "\n",
    "        return path\n",
    "\n",
    "    def restore_checkpoint(self, checkpoints: dict) -> None:\n",
    "        \"\"\"\n",
    "        The method of loading trained PyTorch model.\n",
    "\n",
    "        :param checkpoints: dictionary which contains {'epoch', 'optimizer_state_dict', 'model_state_dict'}\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self._start_epoch = checkpoints[\"epoch\"]\n",
    "        if not isinstance(self._start_epoch, int):\n",
    "            raise TypeError\n",
    "\n",
    "        if self._is_parallel:\n",
    "            self.model.module.load_state_dict(checkpoints[\"model_state_dict\"])\n",
    "        else:\n",
    "            self.model.load_state_dict(checkpoints[\"model_state_dict\"])\n",
    "\n",
    "        self.optimizer.load_state_dict(checkpoints[\"optimizer_state_dict\"])\n",
    "\n",
    "    def restore_from_file(self, path: str, map_location: str = \"cpu\") -> None:\n",
    "        \"\"\"\n",
    "        The method of loading trained PyTorch model from file.\n",
    "\n",
    "        ::\n",
    "\n",
    "            clf = NeuralNetworkClassifier(\n",
    "                    Network(), nn.CrossEntropyLoss(),\n",
    "                    optim.Adam, optimizer_config, experiment\n",
    "                )\n",
    "            clf.restore_from_file('path/to/trained/weights.pth')\n",
    "\n",
    "        :param path: path to saved directory. : str\n",
    "        :param map_location: default cpu: str\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        checkpoints = torch.load(path, map_location=map_location)\n",
    "        self.restore_checkpoint(checkpoints)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920fbcca-6165-4319-bff3-3956ac746491",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d4a2095-3cc3-47db-aa16-b0e93ef14b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args Values (Hardcoded)\n",
    "data_dir = \"/search-data/evan/data/phenotyping/\" # input Your Data Dir Here Pointing To /in-hospital-mortality\n",
    "timestep = 1.0\n",
    "normalizer_state = None\n",
    "imputation = 'previous'\n",
    "\n",
    "train_reader = PhenotypingReader(\n",
    "    dataset_dir=os.path.join(data_dir, 'train'),\n",
    "    listfile=os.path.join(data_dir, 'train_listfile.csv'),\n",
    ")\n",
    "\n",
    "val_reader = PhenotypingReader(\n",
    "    dataset_dir=os.path.join(data_dir, 'train'),\n",
    "    listfile=os.path.join(data_dir, 'val_listfile.csv'),\n",
    ")\n",
    "\n",
    "discretizer = Discretizer(\n",
    "    timestep=float(timestep),\n",
    "    store_masks=True,\n",
    "    impute_strategy='previous',\n",
    "    start_time='zero'\n",
    ")\n",
    "\n",
    "discretizer_header = discretizer.transform(train_reader.read_example(0)[\"X\"])[1].split(',')\n",
    "cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
    "\n",
    "normalizer = Normalizer(fields=cont_channels)  # choose here which columns to standardize\n",
    "normalizer_state = normalizer_state\n",
    "if normalizer_state is None:\n",
    "    normalizer_state = 'ph_ts{}.input_str-previous.start_time-zero.normalizer'.format(timestep)\n",
    "    normalizer_state = os.path.join(cur_path, normalizer_state)\n",
    "normalizer.load_params(normalizer_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84fb48e2-30f8-41a9-8f5c-a97285e83c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    xs, ys, lengths = zip(*batch)\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    # pad x to [batch, max_len, 76]\n",
    "    padded_x = nn.utils.rnn.pad_sequence(xs, batch_first=True)\n",
    "\n",
    "    # build mask\n",
    "    max_len = padded_x.size(1)\n",
    "    mask = torch.arange(max_len)[None, :] >= lengths[:, None]\n",
    "\n",
    "    # labels to tensor\n",
    "    ys = torch.tensor(ys)\n",
    "\n",
    "    return padded_x, ys, mask\n",
    "\n",
    "class VarLenDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data                # list of np arrays, each shape (seq_len, 76)\n",
    "        self.labels = labels            # 1D tensor or list of ints/floats\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx], dtype=torch.float32)  # convert to tensor\n",
    "        y = torch.tensor(self.labels[idx])\n",
    "        length = x.size(0)\n",
    "        return x, y, length\n",
    "\n",
    "def load_data(reader, discretizer, normalizer, small_part=False):\n",
    "    N = reader.get_number_of_examples()\n",
    "    if small_part:\n",
    "        N = 1000\n",
    "\n",
    "    ret = common_utils.read_chunk(reader, N)\n",
    "    data = ret[\"X\"]\n",
    "    ts = ret[\"t\"]\n",
    "    ys = ret[\"y\"]\n",
    "    names = ret[\"name\"]\n",
    "\n",
    "    # Apply discretizer and normalizer\n",
    "    data = [discretizer.transform(X, end=t)[0] for (X, t) in zip(data, ts)]\n",
    "    if normalizer is not None:\n",
    "        data = [normalizer.transform(X) for X in data]\n",
    "\n",
    "    # Convert labels to array\n",
    "    ys = np.array(ys, dtype=np.int32)\n",
    "    ys_int = np.argmax(ys, axis=1)\n",
    "    \n",
    "    dataset = VarLenDataset(data, ys_int)    \n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "408f7408-fa98-4069-b9e5-553bcec7902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_part = False\n",
    "\n",
    "train_ds = load_data(train_reader, discretizer, normalizer, small_part=small_part)\n",
    "val_ds = load_data(val_reader, discretizer, normalizer, small_part=small_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b0ee116-5e27-4459-a496-69388f9c3171",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reader = PhenotypingReader(\n",
    "    dataset_dir=os.path.join(data_dir, 'test'),\n",
    "    listfile=os.path.join(data_dir, 'test_listfile.csv')\n",
    ")\n",
    "\n",
    "test_ds = load_data(\n",
    "    test_reader, \n",
    "    discretizer, \n",
    "    normalizer, \n",
    "    small_part=small_part\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33bbf1f5-c9b5-4661-a525-1f02edd375aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(train_x, f\"{data_dir}/phenotype_train_x.pt\")\n",
    "# torch.save(train_y, f\"{data_dir}/phenotype_train_y.pt\")\n",
    "# torch.save(train_mask, f\"{data_dir}/phenotype_train_mask.pt\")\n",
    "# torch.save(val_x,   f\"{data_dir}/phenotype_val_x.pt\")\n",
    "# torch.save(val_y,   f\"{data_dir}/phenotype_val_y.pt\")\n",
    "# torch.save(val_mask,   f\"{data_dir}/phenotype_val_mask.pt\")\n",
    "# torch.save(test_x, f\"{data_dir}/phenotype_test_x.pt\")\n",
    "# torch.save(test_y, f\"{data_dir}/phenotype_test_y.pt\")\n",
    "# torch.save(test_mask, f\"{data_dir}/phenotype_test_mask.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3b233a-3377-4049-8132-5581468c90dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "088aee3c-fd26-4005-9b5d-135611eab593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "in_feature = 76\n",
    "n_heads = 8 # Number of heads for multi-head attention layer: Should be fixed at 8\n",
    "num_class = 25 # Number of output class\n",
    "num_layers = 2 # Number of multi-head attention layers (N): This depends on the task at hand\n",
    "d_model = 256 # Original 256\n",
    "dropout_rate = 0.4\n",
    "optimizer_config = {\n",
    "    'lr' : 0.0005,\n",
    "    'betas' : (0.9, 0.98),\n",
    "    'eps' : 1e-08,\n",
    "}\n",
    "num_epochs = 40\n",
    "batch_size = 32 # Original 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ef57617-11fa-4363-a6a8-40f926f5c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5535f4a-8fce-4764-bd4f-2fec63272f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/mimic3-sand/wandb/run-20251118_013944-rfsj5oph</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/millikanevan-personal/sand-mimic3/runs/rfsj5oph' target=\"_blank\">glowing-night-80</a></strong> to <a href='https://wandb.ai/millikanevan-personal/sand-mimic3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/millikanevan-personal/sand-mimic3' target=\"_blank\">https://wandb.ai/millikanevan-personal/sand-mimic3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/millikanevan-personal/sand-mimic3/runs/rfsj5oph' target=\"_blank\">https://wandb.ai/millikanevan-personal/sand-mimic3/runs/rfsj5oph</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sand = SAnD(\n",
    "    input_features = in_feature,\n",
    "    n_heads = n_heads,\n",
    "    n_class = num_class, \n",
    "    n_layers = num_layers, \n",
    "    d_model = d_model,\n",
    "    dropout_rate = dropout_rate\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "model = NeuralNetworkClassifier(\n",
    "    sand,\n",
    "    nn.CrossEntropyLoss(),\n",
    "    optim.Adam,\n",
    "    optimizer_config=optimizer_config,\n",
    "    experiment=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152e8de-5d20-49c4-956d-7663d7945cab",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/29250 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   0%|          | 0/29250 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   0%|          | 32/29250 [00:00<06:45, 72.00it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   0%|          | 32/29250 [00:02<06:45, 72.00it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   0%|          | 64/29250 [00:02<21:14, 22.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   0%|          | 64/29250 [00:03<21:14, 22.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   0%|          | 96/29250 [00:03<14:57, 32.49it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   0%|          | 96/29250 [00:04<14:57, 32.49it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   0%|          | 128/29250 [00:04<16:03, 30.21it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   0%|          | 128/29250 [00:07<16:03, 30.21it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   1%|          | 160/29250 [00:07<26:42, 18.15it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   1%|          | 160/29250 [00:07<26:42, 18.15it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   1%|          | 192/29250 [00:07<21:14, 22.80it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   1%|          | 192/29250 [00:08<21:14, 22.80it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   1%|          | 224/29250 [00:08<18:31, 26.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   1%|          | 224/29250 [00:09<18:31, 26.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   1%|          | 256/29250 [00:09<14:35, 33.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   1%|          | 256/29250 [00:09<14:35, 33.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   1%|          | 288/29250 [00:09<13:57, 34.59it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   1%|          | 288/29250 [00:10<13:57, 34.59it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   1%|          | 320/29250 [00:10<11:37, 41.46it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   1%|          | 320/29250 [00:11<11:37, 41.46it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   1%|          | 352/29250 [00:11<11:49, 40.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   1%|          | 352/29250 [00:12<11:49, 40.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   1%|▏         | 384/29250 [00:12<15:53, 30.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   1%|▏         | 384/29250 [00:13<15:53, 30.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   1%|▏         | 416/29250 [00:13<14:14, 33.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   1%|▏         | 416/29250 [00:14<14:14, 33.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   2%|▏         | 448/29250 [00:14<12:57, 37.06it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   2%|▏         | 448/29250 [00:15<12:57, 37.06it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   2%|▏         | 480/29250 [00:15<12:17, 38.99it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   2%|▏         | 480/29250 [00:15<12:17, 38.99it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   2%|▏         | 512/29250 [00:15<10:54, 43.90it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   2%|▏         | 512/29250 [00:16<10:54, 43.90it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   2%|▏         | 544/29250 [00:16<11:03, 43.25it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   2%|▏         | 544/29250 [00:16<11:03, 43.25it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   2%|▏         | 576/29250 [00:16<10:03, 47.48it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   2%|▏         | 576/29250 [00:17<10:03, 47.48it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   2%|▏         | 608/29250 [00:17<08:44, 54.64it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   2%|▏         | 608/29250 [00:17<08:44, 54.64it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   2%|▏         | 640/29250 [00:17<07:16, 65.56it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   2%|▏         | 640/29250 [00:17<07:16, 65.56it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   2%|▏         | 672/29250 [00:17<07:13, 65.88it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   2%|▏         | 672/29250 [00:18<07:13, 65.88it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   2%|▏         | 704/29250 [00:18<06:37, 71.81it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   2%|▏         | 704/29250 [00:18<06:37, 71.81it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   3%|▎         | 736/29250 [00:18<06:31, 72.78it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   3%|▎         | 736/29250 [00:18<06:31, 72.78it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   3%|▎         | 768/29250 [00:18<05:50, 81.20it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   3%|▎         | 768/29250 [00:19<05:50, 81.20it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   3%|▎         | 800/29250 [00:19<05:11, 91.19it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   3%|▎         | 800/29250 [00:19<05:11, 91.19it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   3%|▎         | 832/29250 [00:19<05:04, 93.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   3%|▎         | 832/29250 [00:19<05:04, 93.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   3%|▎         | 864/29250 [00:19<05:11, 91.25it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   3%|▎         | 864/29250 [00:20<05:11, 91.25it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   3%|▎         | 896/29250 [00:20<04:53, 96.50it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   3%|▎         | 896/29250 [00:20<04:53, 96.50it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   3%|▎         | 928/29250 [00:20<05:35, 84.40it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   3%|▎         | 928/29250 [00:21<05:35, 84.40it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   3%|▎         | 960/29250 [00:21<05:16, 89.44it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   3%|▎         | 960/29250 [00:22<05:16, 89.44it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   3%|▎         | 992/29250 [00:22<10:12, 46.17it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   3%|▎         | 992/29250 [00:22<10:12, 46.17it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▎         | 1024/29250 [00:22<08:11, 57.46it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▎         | 1024/29250 [00:26<08:11, 57.46it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▎         | 1056/29250 [00:26<21:45, 21.60it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▎         | 1056/29250 [00:27<21:45, 21.60it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▎         | 1088/29250 [00:27<21:47, 21.54it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▎         | 1088/29250 [00:28<21:47, 21.54it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▍         | 1120/29250 [00:28<17:53, 26.21it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▍         | 1120/29250 [00:29<17:53, 26.21it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▍         | 1152/29250 [00:29<15:44, 29.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▍         | 1152/29250 [00:29<15:44, 29.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▍         | 1184/29250 [00:29<12:53, 36.28it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▍         | 1184/29250 [00:30<12:53, 36.28it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▍         | 1216/29250 [00:30<12:15, 38.12it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▍         | 1216/29250 [00:33<12:15, 38.12it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▍         | 1248/29250 [00:33<23:55, 19.51it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▍         | 1248/29250 [00:34<23:55, 19.51it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▍         | 1280/29250 [00:34<21:25, 21.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▍         | 1280/29250 [00:36<21:25, 21.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▍         | 1312/29250 [00:36<21:00, 22.17it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   4%|▍         | 1312/29250 [00:36<21:00, 22.17it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   5%|▍         | 1344/29250 [00:36<17:24, 26.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   5%|▍         | 1344/29250 [00:37<17:24, 26.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   5%|▍         | 1376/29250 [00:37<13:41, 33.93it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   5%|▍         | 1376/29250 [00:38<13:41, 33.93it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   5%|▍         | 1408/29250 [00:38<15:07, 30.68it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   5%|▍         | 1408/29250 [00:39<15:07, 30.68it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   5%|▍         | 1440/29250 [00:39<12:46, 36.26it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   5%|▍         | 1440/29250 [00:39<12:46, 36.26it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   5%|▌         | 1472/29250 [00:39<09:49, 47.14it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   5%|▌         | 1472/29250 [00:42<09:49, 47.14it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   5%|▌         | 1504/29250 [00:42<22:12, 20.82it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   5%|▌         | 1504/29250 [00:44<22:12, 20.82it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   5%|▌         | 1536/29250 [00:44<21:24, 21.58it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   5%|▌         | 1536/29250 [00:44<21:24, 21.58it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   5%|▌         | 1568/29250 [00:44<16:25, 28.09it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   5%|▌         | 1568/29250 [00:45<16:25, 28.09it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   5%|▌         | 1600/29250 [00:45<15:29, 29.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   5%|▌         | 1600/29250 [00:46<15:29, 29.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   6%|▌         | 1632/29250 [00:46<17:22, 26.49it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   6%|▌         | 1632/29250 [00:47<17:22, 26.49it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   6%|▌         | 1664/29250 [00:47<14:10, 32.44it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   6%|▌         | 1664/29250 [00:47<14:10, 32.44it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   6%|▌         | 1696/29250 [00:47<12:09, 37.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   6%|▌         | 1696/29250 [00:48<12:09, 37.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   6%|▌         | 1728/29250 [00:48<10:56, 41.92it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   6%|▌         | 1728/29250 [00:49<10:56, 41.92it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   6%|▌         | 1760/29250 [00:49<11:50, 38.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   6%|▌         | 1760/29250 [00:51<11:50, 38.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   6%|▌         | 1792/29250 [00:51<15:05, 30.31it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   6%|▌         | 1792/29250 [00:52<15:05, 30.31it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   6%|▌         | 1824/29250 [00:52<16:11, 28.24it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   6%|▌         | 1824/29250 [00:53<16:11, 28.24it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   6%|▋         | 1856/29250 [00:53<14:46, 30.90it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   6%|▋         | 1856/29250 [00:53<14:46, 30.90it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   6%|▋         | 1888/29250 [00:53<11:36, 39.29it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   6%|▋         | 1888/29250 [00:55<11:36, 39.29it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   7%|▋         | 1920/29250 [00:55<16:40, 27.31it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   7%|▋         | 1920/29250 [00:56<16:40, 27.31it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   7%|▋         | 1952/29250 [00:56<14:40, 31.02it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   7%|▋         | 1952/29250 [00:56<14:40, 31.02it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   7%|▋         | 1984/29250 [00:56<11:17, 40.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   7%|▋         | 1984/29250 [00:56<11:17, 40.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   7%|▋         | 2016/29250 [00:56<09:04, 49.97it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   7%|▋         | 2016/29250 [00:57<09:04, 49.97it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   7%|▋         | 2048/29250 [00:57<08:12, 55.18it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   7%|▋         | 2048/29250 [00:59<08:12, 55.18it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   7%|▋         | 2080/29250 [00:59<15:22, 29.46it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   7%|▋         | 2080/29250 [00:59<15:22, 29.46it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   7%|▋         | 2112/29250 [00:59<12:11, 37.09it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   7%|▋         | 2112/29250 [01:00<12:11, 37.09it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   7%|▋         | 2144/29250 [01:00<10:19, 43.78it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   7%|▋         | 2144/29250 [01:02<10:19, 43.78it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   7%|▋         | 2176/29250 [01:02<18:41, 24.14it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   7%|▋         | 2176/29250 [01:03<18:41, 24.14it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   8%|▊         | 2208/29250 [01:03<15:35, 28.90it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   8%|▊         | 2208/29250 [01:04<15:35, 28.90it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   8%|▊         | 2240/29250 [01:04<13:11, 34.14it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   8%|▊         | 2240/29250 [01:04<13:11, 34.14it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   8%|▊         | 2272/29250 [01:04<11:27, 39.25it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   8%|▊         | 2272/29250 [01:04<11:27, 39.25it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   8%|▊         | 2304/29250 [01:04<08:50, 50.76it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   8%|▊         | 2304/29250 [01:05<08:50, 50.76it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   8%|▊         | 2336/29250 [01:05<10:31, 42.64it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   8%|▊         | 2336/29250 [01:06<10:31, 42.64it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   8%|▊         | 2368/29250 [01:06<08:40, 51.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   8%|▊         | 2368/29250 [01:06<08:40, 51.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   8%|▊         | 2400/29250 [01:06<07:23, 60.56it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   8%|▊         | 2400/29250 [01:07<07:23, 60.56it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   8%|▊         | 2432/29250 [01:07<09:34, 46.66it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   8%|▊         | 2432/29250 [01:08<09:34, 46.66it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   8%|▊         | 2464/29250 [01:08<11:01, 40.50it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   8%|▊         | 2464/29250 [01:08<11:01, 40.50it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   9%|▊         | 2496/29250 [01:08<08:37, 51.67it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   9%|▊         | 2496/29250 [01:09<08:37, 51.67it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   9%|▊         | 2528/29250 [01:09<08:19, 53.51it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   9%|▊         | 2528/29250 [01:09<08:19, 53.51it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   9%|▉         | 2560/29250 [01:09<06:38, 67.01it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   9%|▉         | 2560/29250 [01:09<06:38, 67.01it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   9%|▉         | 2592/29250 [01:09<05:21, 82.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   9%|▉         | 2592/29250 [01:11<05:21, 82.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   9%|▉         | 2624/29250 [01:11<09:22, 47.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   9%|▉         | 2624/29250 [01:14<09:22, 47.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   9%|▉         | 2656/29250 [01:14<22:46, 19.46it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   9%|▉         | 2656/29250 [01:15<22:46, 19.46it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   9%|▉         | 2688/29250 [01:15<16:51, 26.26it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   9%|▉         | 2688/29250 [01:16<16:51, 26.26it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   9%|▉         | 2720/29250 [01:16<15:50, 27.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   9%|▉         | 2720/29250 [01:16<15:50, 27.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   9%|▉         | 2752/29250 [01:16<12:20, 35.80it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:   9%|▉         | 2752/29250 [01:16<12:20, 35.80it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  10%|▉         | 2784/29250 [01:16<09:18, 47.40it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  10%|▉         | 2784/29250 [01:16<09:18, 47.40it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  10%|▉         | 2816/29250 [01:16<07:58, 55.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  10%|▉         | 2816/29250 [01:17<07:58, 55.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  10%|▉         | 2848/29250 [01:17<07:47, 56.48it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  10%|▉         | 2848/29250 [01:17<07:47, 56.48it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  10%|▉         | 2880/29250 [01:17<06:38, 66.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  10%|▉         | 2880/29250 [01:18<06:38, 66.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  10%|▉         | 2912/29250 [01:18<05:33, 79.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  10%|▉         | 2912/29250 [01:18<05:33, 79.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  10%|█         | 2944/29250 [01:18<06:47, 64.50it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  10%|█         | 2944/29250 [01:19<06:47, 64.50it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  10%|█         | 2976/29250 [01:19<07:12, 60.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  10%|█         | 2976/29250 [01:19<07:12, 60.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  10%|█         | 3008/29250 [01:19<06:44, 64.94it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  10%|█         | 3008/29250 [01:20<06:44, 64.94it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  10%|█         | 3040/29250 [01:20<05:52, 74.28it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  10%|█         | 3040/29250 [01:20<05:52, 74.28it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█         | 3072/29250 [01:20<05:53, 74.04it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█         | 3072/29250 [01:21<05:53, 74.04it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█         | 3104/29250 [01:21<09:04, 48.04it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█         | 3104/29250 [01:23<09:04, 48.04it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█         | 3136/29250 [01:23<15:05, 28.84it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█         | 3136/29250 [01:24<15:05, 28.84it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█         | 3168/29250 [01:24<12:57, 33.56it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█         | 3168/29250 [01:25<12:57, 33.56it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█         | 3200/29250 [01:25<11:56, 36.34it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█         | 3200/29250 [01:26<11:56, 36.34it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█         | 3232/29250 [01:26<12:28, 34.76it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█         | 3232/29250 [01:27<12:28, 34.76it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█         | 3264/29250 [01:27<13:34, 31.89it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█         | 3264/29250 [01:27<13:34, 31.89it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█▏        | 3296/29250 [01:27<10:27, 41.39it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█▏        | 3296/29250 [01:28<10:27, 41.39it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█▏        | 3328/29250 [01:28<09:39, 44.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█▏        | 3328/29250 [01:29<09:39, 44.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█▏        | 3360/29250 [01:29<11:22, 37.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  11%|█▏        | 3360/29250 [01:29<11:22, 37.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  12%|█▏        | 3392/29250 [01:29<08:46, 49.08it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  12%|█▏        | 3392/29250 [01:30<08:46, 49.08it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  12%|█▏        | 3424/29250 [01:30<11:41, 36.80it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  12%|█▏        | 3424/29250 [01:31<11:41, 36.80it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  12%|█▏        | 3456/29250 [01:31<10:58, 39.18it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  12%|█▏        | 3456/29250 [01:33<10:58, 39.18it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  12%|█▏        | 3488/29250 [01:33<15:24, 27.86it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  12%|█▏        | 3488/29250 [01:34<15:24, 27.86it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  12%|█▏        | 3520/29250 [01:34<13:45, 31.19it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  12%|█▏        | 3520/29250 [01:34<13:45, 31.19it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  12%|█▏        | 3552/29250 [01:34<12:31, 34.21it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  12%|█▏        | 3552/29250 [01:36<12:31, 34.21it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  12%|█▏        | 3584/29250 [01:36<15:21, 27.84it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  12%|█▏        | 3584/29250 [01:37<15:21, 27.84it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  12%|█▏        | 3616/29250 [01:37<14:49, 28.81it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  12%|█▏        | 3616/29250 [01:39<14:49, 28.81it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  12%|█▏        | 3648/29250 [01:39<18:26, 23.13it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  12%|█▏        | 3648/29250 [01:40<18:26, 23.13it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  13%|█▎        | 3680/29250 [01:40<16:29, 25.84it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  13%|█▎        | 3680/29250 [01:41<16:29, 25.84it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  13%|█▎        | 3712/29250 [01:41<13:35, 31.31it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  13%|█▎        | 3712/29250 [01:41<13:35, 31.31it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  13%|█▎        | 3744/29250 [01:41<11:21, 37.44it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  13%|█▎        | 3744/29250 [01:42<11:21, 37.44it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  13%|█▎        | 3776/29250 [01:42<10:29, 40.48it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  13%|█▎        | 3776/29250 [01:42<10:29, 40.48it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  13%|█▎        | 3808/29250 [01:42<10:16, 41.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  13%|█▎        | 3808/29250 [01:43<10:16, 41.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  13%|█▎        | 3840/29250 [01:43<09:11, 46.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  13%|█▎        | 3840/29250 [01:43<09:11, 46.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  13%|█▎        | 3872/29250 [01:43<08:26, 50.06it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  13%|█▎        | 3872/29250 [01:44<08:26, 50.06it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  13%|█▎        | 3904/29250 [01:44<06:36, 63.89it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  13%|█▎        | 3904/29250 [01:44<06:36, 63.89it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  13%|█▎        | 3936/29250 [01:44<07:24, 56.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  13%|█▎        | 3936/29250 [01:45<07:24, 56.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  14%|█▎        | 3968/29250 [01:45<06:53, 61.12it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  14%|█▎        | 3968/29250 [01:47<06:53, 61.12it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  14%|█▎        | 4000/29250 [01:47<12:20, 34.08it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  14%|█▎        | 4000/29250 [01:47<12:20, 34.08it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  14%|█▍        | 4032/29250 [01:47<10:18, 40.78it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  14%|█▍        | 4032/29250 [01:47<10:18, 40.78it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  14%|█▍        | 4064/29250 [01:47<08:18, 50.53it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  14%|█▍        | 4064/29250 [01:48<08:18, 50.53it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  14%|█▍        | 4096/29250 [01:48<07:23, 56.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  14%|█▍        | 4096/29250 [01:52<07:23, 56.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  14%|█▍        | 4128/29250 [01:52<20:39, 20.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  14%|█▍        | 4128/29250 [01:52<20:39, 20.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  14%|█▍        | 4160/29250 [01:52<16:02, 26.08it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  14%|█▍        | 4160/29250 [01:52<16:02, 26.08it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  14%|█▍        | 4192/29250 [01:52<12:02, 34.70it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  14%|█▍        | 4192/29250 [01:54<12:02, 34.70it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  14%|█▍        | 4224/29250 [01:54<14:39, 28.46it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  14%|█▍        | 4224/29250 [01:54<14:39, 28.46it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  15%|█▍        | 4256/29250 [01:54<11:34, 35.98it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  15%|█▍        | 4256/29250 [01:57<11:34, 35.98it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  15%|█▍        | 4288/29250 [01:57<20:35, 20.20it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  15%|█▍        | 4288/29250 [01:58<20:35, 20.20it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  15%|█▍        | 4320/29250 [01:58<15:48, 26.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  15%|█▍        | 4320/29250 [01:59<15:48, 26.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  15%|█▍        | 4352/29250 [01:59<14:20, 28.92it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  15%|█▍        | 4352/29250 [01:59<14:20, 28.92it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  15%|█▍        | 4384/29250 [01:59<12:32, 33.03it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  15%|█▍        | 4384/29250 [02:01<12:32, 33.03it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  15%|█▌        | 4416/29250 [02:01<15:12, 27.21it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  15%|█▌        | 4416/29250 [02:02<15:12, 27.21it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  15%|█▌        | 4448/29250 [02:02<15:59, 25.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  15%|█▌        | 4448/29250 [02:03<15:59, 25.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  15%|█▌        | 4480/29250 [02:03<14:17, 28.89it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  15%|█▌        | 4480/29250 [02:05<14:17, 28.89it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  15%|█▌        | 4512/29250 [02:05<15:14, 27.06it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  15%|█▌        | 4512/29250 [02:06<15:14, 27.06it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  16%|█▌        | 4544/29250 [02:06<14:26, 28.51it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  16%|█▌        | 4544/29250 [02:06<14:26, 28.51it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  16%|█▌        | 4576/29250 [02:06<10:42, 38.38it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  16%|█▌        | 4576/29250 [02:06<10:42, 38.38it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  16%|█▌        | 4608/29250 [02:06<08:32, 48.04it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  16%|█▌        | 4608/29250 [02:07<08:32, 48.04it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  16%|█▌        | 4640/29250 [02:07<09:52, 41.51it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  16%|█▌        | 4640/29250 [02:07<09:52, 41.51it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  16%|█▌        | 4672/29250 [02:07<08:34, 47.78it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  16%|█▌        | 4672/29250 [02:09<08:34, 47.78it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  16%|█▌        | 4704/29250 [02:09<10:37, 38.48it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  16%|█▌        | 4704/29250 [02:10<10:37, 38.48it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  16%|█▌        | 4736/29250 [02:10<10:54, 37.44it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  16%|█▌        | 4736/29250 [02:10<10:54, 37.44it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  16%|█▋        | 4768/29250 [02:10<09:23, 43.43it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  16%|█▋        | 4768/29250 [02:11<09:23, 43.43it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  16%|█▋        | 4800/29250 [02:11<08:37, 47.23it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  16%|█▋        | 4800/29250 [02:11<08:37, 47.23it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  17%|█▋        | 4832/29250 [02:11<08:28, 48.05it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  17%|█▋        | 4832/29250 [02:12<08:28, 48.05it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  17%|█▋        | 4864/29250 [02:12<07:25, 54.70it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  17%|█▋        | 4864/29250 [02:12<07:25, 54.70it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  17%|█▋        | 4896/29250 [02:12<05:54, 68.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  17%|█▋        | 4896/29250 [02:12<05:54, 68.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  17%|█▋        | 4928/29250 [02:12<06:05, 66.61it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  17%|█▋        | 4928/29250 [02:13<06:05, 66.61it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  17%|█▋        | 4960/29250 [02:13<06:37, 61.05it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  17%|█▋        | 4960/29250 [02:14<06:37, 61.05it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  17%|█▋        | 4992/29250 [02:14<08:17, 48.79it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  17%|█▋        | 4992/29250 [02:15<08:17, 48.79it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  17%|█▋        | 5024/29250 [02:15<09:39, 41.77it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  17%|█▋        | 5024/29250 [02:19<09:39, 41.77it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  17%|█▋        | 5056/29250 [02:19<23:04, 17.47it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  17%|█▋        | 5056/29250 [02:20<23:04, 17.47it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  17%|█▋        | 5088/29250 [02:20<18:07, 22.23it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  17%|█▋        | 5088/29250 [02:20<18:07, 22.23it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5120/29250 [02:20<15:14, 26.37it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5120/29250 [02:21<15:14, 26.37it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5152/29250 [02:21<11:53, 33.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5152/29250 [02:23<11:53, 33.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5184/29250 [02:23<15:34, 25.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5184/29250 [02:23<15:34, 25.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5216/29250 [02:23<13:16, 30.17it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5216/29250 [02:30<13:16, 30.17it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5248/29250 [02:30<36:07, 11.08it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5248/29250 [02:31<36:07, 11.08it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5280/29250 [02:31<26:45, 14.93it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5280/29250 [02:31<26:45, 14.93it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5312/29250 [02:31<20:02, 19.90it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5312/29250 [02:32<20:02, 19.90it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5344/29250 [02:32<15:24, 25.87it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5344/29250 [02:33<15:24, 25.87it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5376/29250 [02:33<16:00, 24.86it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5376/29250 [02:38<16:00, 24.86it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5408/29250 [02:38<28:38, 13.88it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  18%|█▊        | 5408/29250 [02:39<28:38, 13.88it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  19%|█▊        | 5440/29250 [02:39<23:11, 17.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  19%|█▊        | 5440/29250 [02:39<23:11, 17.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  19%|█▊        | 5472/29250 [02:39<18:37, 21.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  19%|█▊        | 5472/29250 [02:40<18:37, 21.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  19%|█▉        | 5504/29250 [02:40<17:16, 22.90it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  19%|█▉        | 5504/29250 [02:42<17:16, 22.90it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  19%|█▉        | 5536/29250 [02:42<17:22, 22.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  19%|█▉        | 5536/29250 [02:42<17:22, 22.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  19%|█▉        | 5568/29250 [02:42<12:46, 30.89it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  19%|█▉        | 5568/29250 [02:42<12:46, 30.89it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  19%|█▉        | 5600/29250 [02:42<09:35, 41.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  19%|█▉        | 5600/29250 [02:43<09:35, 41.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  19%|█▉        | 5632/29250 [02:43<08:42, 45.21it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  19%|█▉        | 5632/29250 [02:43<08:42, 45.21it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  19%|█▉        | 5664/29250 [02:43<07:27, 52.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  19%|█▉        | 5664/29250 [02:44<07:27, 52.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  19%|█▉        | 5696/29250 [02:44<08:19, 47.20it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  19%|█▉        | 5696/29250 [02:44<08:19, 47.20it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  20%|█▉        | 5728/29250 [02:44<06:27, 60.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  20%|█▉        | 5728/29250 [02:45<06:27, 60.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  20%|█▉        | 5760/29250 [02:45<07:10, 54.61it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  20%|█▉        | 5760/29250 [02:46<07:10, 54.61it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  20%|█▉        | 5792/29250 [02:46<10:50, 36.06it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  20%|█▉        | 5792/29250 [02:47<10:50, 36.06it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  20%|█▉        | 5824/29250 [02:47<10:36, 36.82it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  20%|█▉        | 5824/29250 [02:48<10:36, 36.82it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  20%|██        | 5856/29250 [02:48<09:18, 41.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  20%|██        | 5856/29250 [02:48<09:18, 41.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  20%|██        | 5888/29250 [02:48<08:28, 45.95it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  20%|██        | 5888/29250 [02:49<08:28, 45.95it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  20%|██        | 5920/29250 [02:49<10:16, 37.83it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  20%|██        | 5920/29250 [02:50<10:16, 37.83it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  20%|██        | 5952/29250 [02:50<09:36, 40.40it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  20%|██        | 5952/29250 [02:51<09:36, 40.40it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  20%|██        | 5984/29250 [02:51<08:59, 43.09it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  20%|██        | 5984/29250 [02:51<08:59, 43.09it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  21%|██        | 6016/29250 [02:51<08:37, 44.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  21%|██        | 6016/29250 [02:52<08:37, 44.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  21%|██        | 6048/29250 [02:52<08:24, 45.95it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  21%|██        | 6048/29250 [02:53<08:24, 45.95it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  21%|██        | 6080/29250 [02:53<07:39, 50.40it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  21%|██        | 6080/29250 [02:54<07:39, 50.40it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  21%|██        | 6112/29250 [02:54<09:49, 39.25it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  21%|██        | 6112/29250 [02:54<09:49, 39.25it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  21%|██        | 6144/29250 [02:54<09:20, 41.20it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  21%|██        | 6144/29250 [02:55<09:20, 41.20it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  21%|██        | 6176/29250 [02:55<07:36, 50.60it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  21%|██        | 6176/29250 [02:55<07:36, 50.60it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  21%|██        | 6208/29250 [02:55<07:06, 54.00it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  21%|██        | 6208/29250 [02:57<07:06, 54.00it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  21%|██▏       | 6240/29250 [02:57<10:33, 36.33it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  21%|██▏       | 6240/29250 [02:57<10:33, 36.33it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  21%|██▏       | 6272/29250 [02:58<08:33, 44.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  21%|██▏       | 6272/29250 [02:58<08:33, 44.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  22%|██▏       | 6304/29250 [02:58<09:10, 41.65it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  22%|██▏       | 6304/29250 [02:58<09:10, 41.65it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  22%|██▏       | 6336/29250 [02:58<07:48, 48.94it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  22%|██▏       | 6336/29250 [02:59<07:48, 48.94it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  22%|██▏       | 6368/29250 [02:59<06:08, 62.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  22%|██▏       | 6368/29250 [02:59<06:08, 62.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  22%|██▏       | 6400/29250 [02:59<05:02, 75.59it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  22%|██▏       | 6400/29250 [03:00<05:02, 75.59it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  22%|██▏       | 6432/29250 [03:00<06:58, 54.50it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  22%|██▏       | 6432/29250 [03:01<06:58, 54.50it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  22%|██▏       | 6464/29250 [03:01<07:36, 49.92it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  22%|██▏       | 6464/29250 [03:01<07:36, 49.92it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  22%|██▏       | 6496/29250 [03:01<08:14, 46.03it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  22%|██▏       | 6496/29250 [03:02<08:14, 46.03it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  22%|██▏       | 6528/29250 [03:02<09:28, 39.97it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  22%|██▏       | 6528/29250 [03:03<09:28, 39.97it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  22%|██▏       | 6560/29250 [03:03<09:14, 40.93it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  22%|██▏       | 6560/29250 [03:04<09:14, 40.93it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  23%|██▎       | 6592/29250 [03:04<09:16, 40.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  23%|██▎       | 6592/29250 [03:04<09:16, 40.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  23%|██▎       | 6624/29250 [03:04<07:18, 51.55it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  23%|██▎       | 6624/29250 [03:04<07:18, 51.55it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  23%|██▎       | 6656/29250 [03:04<05:50, 64.51it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  23%|██▎       | 6656/29250 [03:05<05:50, 64.51it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  23%|██▎       | 6688/29250 [03:05<05:54, 63.68it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  23%|██▎       | 6688/29250 [03:06<05:54, 63.68it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  23%|██▎       | 6720/29250 [03:06<06:30, 57.65it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  23%|██▎       | 6720/29250 [03:06<06:30, 57.65it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  23%|██▎       | 6752/29250 [03:06<06:12, 60.36it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  23%|██▎       | 6752/29250 [03:07<06:12, 60.36it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  23%|██▎       | 6784/29250 [03:07<09:06, 41.13it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  23%|██▎       | 6784/29250 [03:09<09:06, 41.13it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  23%|██▎       | 6816/29250 [03:09<11:13, 33.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  23%|██▎       | 6816/29250 [03:09<11:13, 33.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  23%|██▎       | 6848/29250 [03:09<10:11, 36.62it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  23%|██▎       | 6848/29250 [03:10<10:11, 36.62it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  24%|██▎       | 6880/29250 [03:10<08:39, 43.04it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  24%|██▎       | 6880/29250 [03:11<08:39, 43.04it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  24%|██▎       | 6912/29250 [03:11<08:39, 42.98it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  24%|██▎       | 6912/29250 [03:11<08:39, 42.98it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  24%|██▎       | 6944/29250 [03:11<08:46, 42.38it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  24%|██▎       | 6944/29250 [03:12<08:46, 42.38it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  24%|██▍       | 6976/29250 [03:12<08:24, 44.19it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  24%|██▍       | 6976/29250 [03:12<08:24, 44.19it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  24%|██▍       | 7008/29250 [03:12<06:32, 56.61it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  24%|██▍       | 7008/29250 [03:13<06:32, 56.61it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  24%|██▍       | 7040/29250 [03:13<05:49, 63.52it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  24%|██▍       | 7040/29250 [03:13<05:49, 63.52it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  24%|██▍       | 7072/29250 [03:13<04:59, 74.05it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  24%|██▍       | 7072/29250 [03:13<04:59, 74.05it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  24%|██▍       | 7104/29250 [03:13<04:52, 75.62it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  24%|██▍       | 7104/29250 [03:14<04:52, 75.62it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  24%|██▍       | 7136/29250 [03:14<04:55, 74.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  24%|██▍       | 7136/29250 [03:14<04:55, 74.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▍       | 7168/29250 [03:14<04:44, 77.73it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▍       | 7168/29250 [03:15<04:44, 77.73it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▍       | 7200/29250 [03:15<06:36, 55.67it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▍       | 7200/29250 [03:18<06:36, 55.67it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▍       | 7232/29250 [03:18<14:56, 24.55it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▍       | 7232/29250 [03:20<14:56, 24.55it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▍       | 7264/29250 [03:20<17:43, 20.68it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▍       | 7264/29250 [03:22<17:43, 20.68it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▍       | 7296/29250 [03:22<17:20, 21.10it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▍       | 7296/29250 [03:22<17:20, 21.10it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▌       | 7328/29250 [03:22<13:33, 26.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▌       | 7328/29250 [03:25<13:33, 26.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▌       | 7360/29250 [03:25<18:39, 19.56it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▌       | 7360/29250 [03:25<18:39, 19.56it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▌       | 7392/29250 [03:25<15:17, 23.81it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▌       | 7392/29250 [03:26<15:17, 23.81it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▌       | 7424/29250 [03:26<11:35, 31.38it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▌       | 7424/29250 [03:35<11:35, 31.38it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▌       | 7456/29250 [03:35<38:25,  9.45it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  25%|██▌       | 7456/29250 [03:35<38:25,  9.45it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  26%|██▌       | 7488/29250 [03:35<27:59, 12.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  26%|██▌       | 7488/29250 [03:36<27:59, 12.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  26%|██▌       | 7520/29250 [03:36<22:10, 16.33it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  26%|██▌       | 7520/29250 [03:36<22:10, 16.33it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  26%|██▌       | 7552/29250 [03:36<16:39, 21.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  26%|██▌       | 7552/29250 [03:37<16:39, 21.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  26%|██▌       | 7584/29250 [03:37<14:27, 24.98it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  26%|██▌       | 7584/29250 [03:37<14:27, 24.98it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  26%|██▌       | 7616/29250 [03:37<10:40, 33.79it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  26%|██▌       | 7616/29250 [03:38<10:40, 33.79it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  26%|██▌       | 7648/29250 [03:38<11:06, 32.40it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  26%|██▌       | 7648/29250 [03:39<11:06, 32.40it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  26%|██▋       | 7680/29250 [03:39<09:13, 38.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  26%|██▋       | 7680/29250 [03:39<09:13, 38.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  26%|██▋       | 7712/29250 [03:39<08:42, 41.21it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  26%|██▋       | 7712/29250 [03:40<08:42, 41.21it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  26%|██▋       | 7744/29250 [03:40<10:01, 35.77it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  26%|██▋       | 7744/29250 [03:42<10:01, 35.77it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  27%|██▋       | 7776/29250 [03:42<12:15, 29.18it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  27%|██▋       | 7776/29250 [03:42<12:15, 29.18it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  27%|██▋       | 7808/29250 [03:42<10:02, 35.57it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  27%|██▋       | 7808/29250 [03:43<10:02, 35.57it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  27%|██▋       | 7840/29250 [03:43<07:46, 45.94it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  27%|██▋       | 7840/29250 [03:43<07:46, 45.94it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  27%|██▋       | 7872/29250 [03:43<06:10, 57.77it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  27%|██▋       | 7872/29250 [03:44<06:10, 57.77it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  27%|██▋       | 7904/29250 [03:44<06:54, 51.45it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  27%|██▋       | 7904/29250 [03:48<06:54, 51.45it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  27%|██▋       | 7936/29250 [03:48<18:32, 19.16it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  27%|██▋       | 7936/29250 [03:48<18:32, 19.16it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  27%|██▋       | 7968/29250 [03:48<14:41, 24.13it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  27%|██▋       | 7968/29250 [03:48<14:41, 24.13it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  27%|██▋       | 8000/29250 [03:48<10:55, 32.40it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  27%|██▋       | 8000/29250 [03:49<10:55, 32.40it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  27%|██▋       | 8032/29250 [03:49<09:24, 37.61it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  27%|██▋       | 8032/29250 [03:49<09:24, 37.61it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  28%|██▊       | 8064/29250 [03:49<07:43, 45.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  28%|██▊       | 8064/29250 [03:50<07:43, 45.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  28%|██▊       | 8096/29250 [03:50<07:30, 46.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  28%|██▊       | 8096/29250 [03:51<07:30, 46.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  28%|██▊       | 8128/29250 [03:51<07:07, 49.35it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  28%|██▊       | 8128/29250 [03:51<07:07, 49.35it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  28%|██▊       | 8160/29250 [03:51<06:58, 50.44it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  28%|██▊       | 8160/29250 [03:52<06:58, 50.44it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  28%|██▊       | 8192/29250 [03:52<06:21, 55.14it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  28%|██▊       | 8192/29250 [03:52<06:21, 55.14it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  28%|██▊       | 8224/29250 [03:52<06:33, 53.40it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  28%|██▊       | 8224/29250 [03:53<06:33, 53.40it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  28%|██▊       | 8256/29250 [03:53<06:44, 51.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  28%|██▊       | 8256/29250 [03:53<06:44, 51.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  28%|██▊       | 8288/29250 [03:53<06:24, 54.48it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  28%|██▊       | 8288/29250 [03:54<06:24, 54.48it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  28%|██▊       | 8320/29250 [03:54<07:10, 48.57it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  28%|██▊       | 8320/29250 [03:54<07:10, 48.57it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  29%|██▊       | 8352/29250 [03:54<05:39, 61.60it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  29%|██▊       | 8352/29250 [03:55<05:39, 61.60it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  29%|██▊       | 8384/29250 [03:55<07:07, 48.76it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  29%|██▊       | 8384/29250 [03:58<07:07, 48.76it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  29%|██▉       | 8416/29250 [03:58<12:03, 28.79it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  29%|██▉       | 8416/29250 [03:59<12:03, 28.79it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  29%|██▉       | 8448/29250 [03:59<12:53, 26.90it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  29%|██▉       | 8448/29250 [04:00<12:53, 26.90it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  29%|██▉       | 8480/29250 [04:00<10:54, 31.73it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  29%|██▉       | 8480/29250 [04:00<10:54, 31.73it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  29%|██▉       | 8512/29250 [04:00<10:14, 33.72it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  29%|██▉       | 8512/29250 [04:01<10:14, 33.72it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  29%|██▉       | 8544/29250 [04:01<10:25, 33.12it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  29%|██▉       | 8544/29250 [04:02<10:25, 33.12it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  29%|██▉       | 8576/29250 [04:02<09:03, 38.02it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  29%|██▉       | 8576/29250 [04:02<09:03, 38.02it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  29%|██▉       | 8608/29250 [04:02<08:19, 41.36it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  29%|██▉       | 8608/29250 [04:03<08:19, 41.36it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  30%|██▉       | 8640/29250 [04:03<06:55, 49.65it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  30%|██▉       | 8640/29250 [04:04<06:55, 49.65it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  30%|██▉       | 8672/29250 [04:04<07:47, 44.06it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  30%|██▉       | 8672/29250 [04:04<07:47, 44.06it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  30%|██▉       | 8704/29250 [04:04<07:05, 48.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  30%|██▉       | 8704/29250 [04:05<07:05, 48.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  30%|██▉       | 8736/29250 [04:05<07:24, 46.19it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  30%|██▉       | 8736/29250 [04:06<07:24, 46.19it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  30%|██▉       | 8768/29250 [04:06<08:02, 42.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  30%|██▉       | 8768/29250 [04:06<08:02, 42.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  30%|███       | 8800/29250 [04:06<06:12, 54.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  30%|███       | 8800/29250 [04:06<06:12, 54.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  30%|███       | 8832/29250 [04:06<05:17, 64.33it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  30%|███       | 8832/29250 [04:07<05:17, 64.33it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  30%|███       | 8864/29250 [04:07<04:17, 79.16it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  30%|███       | 8864/29250 [04:08<04:17, 79.16it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  30%|███       | 8896/29250 [04:08<07:05, 47.86it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  30%|███       | 8896/29250 [04:08<07:05, 47.86it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  31%|███       | 8928/29250 [04:08<05:39, 59.80it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  31%|███       | 8928/29250 [04:09<05:39, 59.80it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  31%|███       | 8960/29250 [04:09<07:48, 43.28it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  31%|███       | 8960/29250 [04:12<07:48, 43.28it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  31%|███       | 8992/29250 [04:12<15:24, 21.92it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  31%|███       | 8992/29250 [04:14<15:24, 21.92it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  31%|███       | 9024/29250 [04:14<17:06, 19.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  31%|███       | 9024/29250 [04:16<17:06, 19.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  31%|███       | 9056/29250 [04:16<15:32, 21.66it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  31%|███       | 9056/29250 [04:16<15:32, 21.66it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  31%|███       | 9088/29250 [04:16<13:13, 25.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  31%|███       | 9088/29250 [04:17<13:13, 25.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  31%|███       | 9120/29250 [04:17<09:56, 33.77it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  31%|███       | 9120/29250 [04:18<09:56, 33.77it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  31%|███▏      | 9152/29250 [04:18<10:11, 32.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  31%|███▏      | 9152/29250 [04:18<10:11, 32.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  31%|███▏      | 9184/29250 [04:18<07:42, 43.38it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  31%|███▏      | 9184/29250 [04:18<07:42, 43.38it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9216/29250 [04:18<06:58, 47.89it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9216/29250 [04:19<06:58, 47.89it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9248/29250 [04:19<05:39, 58.87it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9248/29250 [04:24<05:39, 58.87it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9280/29250 [04:24<20:33, 16.19it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9280/29250 [04:25<20:33, 16.19it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9312/29250 [04:25<16:34, 20.05it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9312/29250 [04:27<16:34, 20.05it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9344/29250 [04:27<19:57, 16.62it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9344/29250 [04:28<19:57, 16.62it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9376/29250 [04:28<15:00, 22.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9376/29250 [04:31<15:00, 22.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9408/29250 [04:31<21:36, 15.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9408/29250 [04:32<21:36, 15.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9440/29250 [04:32<18:16, 18.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9440/29250 [04:36<18:16, 18.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9472/29250 [04:36<23:41, 13.92it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9472/29250 [04:37<23:41, 13.92it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9504/29250 [04:37<18:55, 17.38it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  32%|███▏      | 9504/29250 [04:37<18:55, 17.38it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  33%|███▎      | 9536/29250 [04:37<15:50, 20.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  33%|███▎      | 9536/29250 [04:38<15:50, 20.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  33%|███▎      | 9568/29250 [04:38<13:56, 23.52it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  33%|███▎      | 9568/29250 [04:39<13:56, 23.52it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  33%|███▎      | 9600/29250 [04:39<10:42, 30.56it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  33%|███▎      | 9600/29250 [04:40<10:42, 30.56it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  33%|███▎      | 9632/29250 [04:40<12:01, 27.18it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  33%|███▎      | 9632/29250 [04:41<12:01, 27.18it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  33%|███▎      | 9664/29250 [04:41<09:55, 32.88it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  33%|███▎      | 9664/29250 [04:41<09:55, 32.88it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  33%|███▎      | 9696/29250 [04:41<08:52, 36.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  33%|███▎      | 9696/29250 [04:42<08:52, 36.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  33%|███▎      | 9728/29250 [04:42<08:08, 39.95it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  33%|███▎      | 9728/29250 [04:42<08:08, 39.95it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  33%|███▎      | 9760/29250 [04:42<07:15, 44.78it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  33%|███▎      | 9760/29250 [04:44<07:15, 44.78it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  33%|███▎      | 9792/29250 [04:44<09:19, 34.76it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  33%|███▎      | 9792/29250 [04:44<09:19, 34.76it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  34%|███▎      | 9824/29250 [04:44<08:30, 38.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  34%|███▎      | 9824/29250 [04:45<08:30, 38.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  34%|███▎      | 9856/29250 [04:45<07:01, 45.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  34%|███▎      | 9856/29250 [04:45<07:01, 45.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  34%|███▍      | 9888/29250 [04:45<06:54, 46.67it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  34%|███▍      | 9888/29250 [04:46<06:54, 46.67it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  34%|███▍      | 9920/29250 [04:46<07:29, 42.98it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  34%|███▍      | 9920/29250 [04:46<07:29, 42.98it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  34%|███▍      | 9952/29250 [04:46<05:40, 56.64it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  34%|███▍      | 9952/29250 [04:47<05:40, 56.64it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  34%|███▍      | 9984/29250 [04:47<04:31, 70.83it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  34%|███▍      | 9984/29250 [04:48<04:31, 70.83it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  34%|███▍      | 10016/29250 [04:48<06:57, 46.12it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  34%|███▍      | 10016/29250 [04:48<06:57, 46.12it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  34%|███▍      | 10048/29250 [04:48<06:12, 51.54it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  34%|███▍      | 10048/29250 [04:50<06:12, 51.54it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  34%|███▍      | 10080/29250 [04:50<08:25, 37.93it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  34%|███▍      | 10080/29250 [04:50<08:25, 37.93it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  35%|███▍      | 10112/29250 [04:50<07:34, 42.09it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  35%|███▍      | 10112/29250 [04:51<07:34, 42.09it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  35%|███▍      | 10144/29250 [04:51<08:13, 38.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  35%|███▍      | 10144/29250 [04:52<08:13, 38.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  35%|███▍      | 10176/29250 [04:52<07:55, 40.08it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  35%|███▍      | 10176/29250 [04:53<07:55, 40.08it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  35%|███▍      | 10208/29250 [04:53<07:19, 43.32it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  35%|███▍      | 10208/29250 [04:53<07:19, 43.32it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  35%|███▌      | 10240/29250 [04:53<05:47, 54.78it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  35%|███▌      | 10240/29250 [04:53<05:47, 54.78it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  35%|███▌      | 10272/29250 [04:53<05:45, 54.87it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  35%|███▌      | 10272/29250 [04:55<05:45, 54.87it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  35%|███▌      | 10304/29250 [04:55<07:54, 39.94it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  35%|███▌      | 10304/29250 [04:55<07:54, 39.94it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  35%|███▌      | 10336/29250 [04:55<06:48, 46.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  35%|███▌      | 10336/29250 [04:56<06:48, 46.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  35%|███▌      | 10368/29250 [04:56<07:37, 41.24it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  35%|███▌      | 10368/29250 [04:57<07:37, 41.24it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  36%|███▌      | 10400/29250 [04:57<08:49, 35.63it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  36%|███▌      | 10400/29250 [04:58<08:49, 35.63it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  36%|███▌      | 10432/29250 [04:58<07:38, 41.08it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  36%|███▌      | 10432/29250 [04:58<07:38, 41.08it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  36%|███▌      | 10464/29250 [04:58<07:05, 44.18it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  36%|███▌      | 10464/29250 [04:59<07:05, 44.18it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  36%|███▌      | 10496/29250 [04:59<07:41, 40.62it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  36%|███▌      | 10496/29250 [05:00<07:41, 40.62it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  36%|███▌      | 10528/29250 [05:00<05:58, 52.23it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  36%|███▌      | 10528/29250 [05:00<05:58, 52.23it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  36%|███▌      | 10560/29250 [05:00<05:58, 52.15it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  36%|███▌      | 10560/29250 [05:01<05:58, 52.15it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  36%|███▌      | 10592/29250 [05:01<06:19, 49.16it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  36%|███▌      | 10592/29250 [05:01<06:19, 49.16it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  36%|███▋      | 10624/29250 [05:01<05:06, 60.81it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  36%|███▋      | 10624/29250 [05:02<05:06, 60.81it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  36%|███▋      | 10656/29250 [05:02<04:34, 67.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  36%|███▋      | 10656/29250 [05:02<04:34, 67.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  37%|███▋      | 10688/29250 [05:02<03:51, 80.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  37%|███▋      | 10688/29250 [05:03<03:51, 80.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  37%|███▋      | 10720/29250 [05:03<05:36, 55.05it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  37%|███▋      | 10720/29250 [05:04<05:36, 55.05it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  37%|███▋      | 10752/29250 [05:04<06:10, 49.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  37%|███▋      | 10752/29250 [05:04<06:10, 49.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  37%|███▋      | 10784/29250 [05:04<04:44, 64.89it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  37%|███▋      | 10784/29250 [05:05<04:44, 64.89it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  37%|███▋      | 10816/29250 [05:05<06:58, 44.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  37%|███▋      | 10816/29250 [05:05<06:58, 44.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  37%|███▋      | 10848/29250 [05:05<05:55, 51.78it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  37%|███▋      | 10848/29250 [05:06<05:55, 51.78it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  37%|███▋      | 10880/29250 [05:06<06:20, 48.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  37%|███▋      | 10880/29250 [05:07<06:20, 48.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  37%|███▋      | 10912/29250 [05:07<08:18, 36.79it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  37%|███▋      | 10912/29250 [05:08<08:18, 36.79it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  37%|███▋      | 10944/29250 [05:08<07:17, 41.81it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  37%|███▋      | 10944/29250 [05:09<07:17, 41.81it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  38%|███▊      | 10976/29250 [05:09<06:53, 44.17it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  38%|███▊      | 10976/29250 [05:11<06:53, 44.17it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  38%|███▊      | 11008/29250 [05:11<11:59, 25.34it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  38%|███▊      | 11008/29250 [05:11<11:59, 25.34it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  38%|███▊      | 11040/29250 [05:11<09:28, 32.03it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  38%|███▊      | 11040/29250 [05:12<09:28, 32.03it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  38%|███▊      | 11072/29250 [05:12<09:27, 32.04it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  38%|███▊      | 11072/29250 [05:13<09:27, 32.04it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  38%|███▊      | 11104/29250 [05:13<08:17, 36.49it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  38%|███▊      | 11104/29250 [05:13<08:17, 36.49it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  38%|███▊      | 11136/29250 [05:13<06:15, 48.24it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  38%|███▊      | 11136/29250 [05:14<06:15, 48.24it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  38%|███▊      | 11168/29250 [05:14<05:10, 58.20it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  38%|███▊      | 11168/29250 [05:16<05:10, 58.20it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  38%|███▊      | 11200/29250 [05:16<09:57, 30.20it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  38%|███▊      | 11200/29250 [05:16<09:57, 30.20it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  38%|███▊      | 11232/29250 [05:16<08:20, 36.01it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  38%|███▊      | 11232/29250 [05:16<08:20, 36.01it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▊      | 11264/29250 [05:16<06:19, 47.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▊      | 11264/29250 [05:17<06:19, 47.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▊      | 11296/29250 [05:17<04:56, 60.55it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▊      | 11296/29250 [05:18<04:56, 60.55it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▊      | 11328/29250 [05:18<07:02, 42.44it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▊      | 11328/29250 [05:19<07:02, 42.44it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▉      | 11360/29250 [05:19<08:28, 35.17it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▉      | 11360/29250 [05:20<08:28, 35.17it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▉      | 11392/29250 [05:20<08:08, 36.59it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▉      | 11392/29250 [05:21<08:08, 36.59it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▉      | 11424/29250 [05:21<09:16, 32.01it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▉      | 11424/29250 [05:21<09:16, 32.01it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▉      | 11456/29250 [05:21<07:00, 42.31it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▉      | 11456/29250 [05:22<07:00, 42.31it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▉      | 11488/29250 [05:22<07:27, 39.72it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▉      | 11488/29250 [05:24<07:27, 39.72it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▉      | 11520/29250 [05:24<10:48, 27.33it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▉      | 11520/29250 [05:25<10:48, 27.33it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▉      | 11552/29250 [05:25<08:06, 36.35it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  39%|███▉      | 11552/29250 [05:25<08:06, 36.35it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  40%|███▉      | 11584/29250 [05:25<06:35, 44.66it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  40%|███▉      | 11584/29250 [05:26<06:35, 44.66it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  40%|███▉      | 11616/29250 [05:26<06:27, 45.48it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  40%|███▉      | 11616/29250 [05:26<06:27, 45.48it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  40%|███▉      | 11648/29250 [05:26<05:51, 50.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  40%|███▉      | 11648/29250 [05:27<05:51, 50.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  40%|███▉      | 11680/29250 [05:27<06:16, 46.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  40%|███▉      | 11680/29250 [05:28<06:16, 46.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  40%|████      | 11712/29250 [05:28<06:10, 47.36it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  40%|████      | 11712/29250 [05:28<06:10, 47.36it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  40%|████      | 11744/29250 [05:28<05:05, 57.33it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  40%|████      | 11744/29250 [05:28<05:05, 57.33it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  40%|████      | 11776/29250 [05:28<05:06, 56.97it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  40%|████      | 11776/29250 [05:29<05:06, 56.97it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  40%|████      | 11808/29250 [05:29<05:14, 55.46it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  40%|████      | 11808/29250 [05:30<05:14, 55.46it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  40%|████      | 11840/29250 [05:30<05:53, 49.29it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  40%|████      | 11840/29250 [05:30<05:53, 49.29it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  41%|████      | 11872/29250 [05:30<05:06, 56.65it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  41%|████      | 11872/29250 [05:31<05:06, 56.65it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  41%|████      | 11904/29250 [05:31<05:19, 54.33it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  41%|████      | 11904/29250 [05:32<05:19, 54.33it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  41%|████      | 11936/29250 [05:32<07:41, 37.55it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  41%|████      | 11936/29250 [05:33<07:41, 37.55it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  41%|████      | 11968/29250 [05:33<06:11, 46.46it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  41%|████      | 11968/29250 [05:33<06:11, 46.46it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  41%|████      | 12000/29250 [05:33<04:48, 59.88it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  41%|████      | 12000/29250 [05:33<04:48, 59.88it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  41%|████      | 12032/29250 [05:33<04:09, 69.04it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  41%|████      | 12032/29250 [05:35<04:09, 69.04it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  41%|████      | 12064/29250 [05:35<08:07, 35.24it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  41%|████      | 12064/29250 [05:36<08:07, 35.24it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  41%|████▏     | 12096/29250 [05:36<07:35, 37.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  41%|████▏     | 12096/29250 [05:37<07:35, 37.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  41%|████▏     | 12128/29250 [05:37<07:42, 37.02it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  41%|████▏     | 12128/29250 [05:37<07:42, 37.02it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  42%|████▏     | 12160/29250 [05:37<06:38, 42.89it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  42%|████▏     | 12160/29250 [05:37<06:38, 42.89it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  42%|████▏     | 12192/29250 [05:37<05:19, 53.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  42%|████▏     | 12192/29250 [05:41<05:19, 53.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  42%|████▏     | 12224/29250 [05:41<12:41, 22.35it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  42%|████▏     | 12224/29250 [05:41<12:41, 22.35it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  42%|████▏     | 12256/29250 [05:41<09:29, 29.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  42%|████▏     | 12256/29250 [05:42<09:29, 29.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  42%|████▏     | 12288/29250 [05:42<09:20, 30.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  42%|████▏     | 12288/29250 [05:42<09:20, 30.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  42%|████▏     | 12320/29250 [05:42<07:02, 40.04it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  42%|████▏     | 12320/29250 [05:43<07:02, 40.04it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  42%|████▏     | 12352/29250 [05:43<07:43, 36.49it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  42%|████▏     | 12352/29250 [05:43<07:43, 36.49it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  42%|████▏     | 12384/29250 [05:43<05:51, 47.94it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  42%|████▏     | 12384/29250 [05:44<05:51, 47.94it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  42%|████▏     | 12416/29250 [05:44<06:13, 45.06it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  42%|████▏     | 12416/29250 [05:45<06:13, 45.06it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  43%|████▎     | 12448/29250 [05:45<05:17, 52.84it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  43%|████▎     | 12448/29250 [05:45<05:17, 52.84it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  43%|████▎     | 12480/29250 [05:45<05:21, 52.17it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  43%|████▎     | 12480/29250 [05:46<05:21, 52.17it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  43%|████▎     | 12512/29250 [05:46<05:53, 47.36it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  43%|████▎     | 12512/29250 [05:50<05:53, 47.36it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  43%|████▎     | 12544/29250 [05:50<13:50, 20.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  43%|████▎     | 12544/29250 [05:51<13:50, 20.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  43%|████▎     | 12576/29250 [05:51<11:45, 23.63it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  43%|████▎     | 12576/29250 [05:51<11:45, 23.63it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  43%|████▎     | 12608/29250 [05:51<09:47, 28.33it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  43%|████▎     | 12608/29250 [05:52<09:47, 28.33it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  43%|████▎     | 12640/29250 [05:52<07:47, 35.51it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  43%|████▎     | 12640/29250 [05:52<07:47, 35.51it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  43%|████▎     | 12672/29250 [05:52<05:51, 47.22it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  43%|████▎     | 12672/29250 [05:59<05:51, 47.22it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  43%|████▎     | 12704/29250 [05:59<22:51, 12.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  43%|████▎     | 12704/29250 [06:00<22:51, 12.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  44%|████▎     | 12736/29250 [06:00<17:21, 15.86it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  44%|████▎     | 12736/29250 [06:00<17:21, 15.86it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  44%|████▎     | 12768/29250 [06:00<13:07, 20.92it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  44%|████▎     | 12768/29250 [06:00<13:07, 20.92it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  44%|████▍     | 12800/29250 [06:00<10:03, 27.25it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  44%|████▍     | 12800/29250 [06:02<10:03, 27.25it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  44%|████▍     | 12832/29250 [06:02<10:14, 26.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  44%|████▍     | 12832/29250 [06:02<10:14, 26.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  44%|████▍     | 12864/29250 [06:02<08:25, 32.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  44%|████▍     | 12864/29250 [06:03<08:25, 32.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  44%|████▍     | 12896/29250 [06:03<08:24, 32.39it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  44%|████▍     | 12896/29250 [06:04<08:24, 32.39it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  44%|████▍     | 12928/29250 [06:04<09:34, 28.43it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  44%|████▍     | 12928/29250 [06:06<09:34, 28.43it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  44%|████▍     | 12960/29250 [06:06<11:36, 23.39it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  44%|████▍     | 12960/29250 [06:07<11:36, 23.39it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  44%|████▍     | 12992/29250 [06:07<08:37, 31.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  44%|████▍     | 12992/29250 [06:07<08:37, 31.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  45%|████▍     | 13024/29250 [06:07<06:58, 38.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  45%|████▍     | 13024/29250 [06:08<06:58, 38.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  45%|████▍     | 13056/29250 [06:08<06:45, 39.94it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  45%|████▍     | 13056/29250 [06:09<06:45, 39.94it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  45%|████▍     | 13088/29250 [06:09<07:41, 34.99it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  45%|████▍     | 13088/29250 [06:10<07:41, 34.99it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  45%|████▍     | 13120/29250 [06:10<07:21, 36.55it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  45%|████▍     | 13120/29250 [06:11<07:21, 36.55it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  45%|████▍     | 13152/29250 [06:11<08:10, 32.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  45%|████▍     | 13152/29250 [06:11<08:10, 32.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  45%|████▌     | 13184/29250 [06:11<06:36, 40.54it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  45%|████▌     | 13184/29250 [06:12<06:36, 40.54it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  45%|████▌     | 13216/29250 [06:12<05:31, 48.35it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  45%|████▌     | 13216/29250 [06:12<05:31, 48.35it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  45%|████▌     | 13248/29250 [06:12<05:01, 53.03it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  45%|████▌     | 13248/29250 [06:13<05:01, 53.03it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  45%|████▌     | 13280/29250 [06:13<06:39, 39.94it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  45%|████▌     | 13280/29250 [06:14<06:39, 39.94it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▌     | 13312/29250 [06:14<05:32, 47.90it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▌     | 13312/29250 [06:14<05:32, 47.90it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▌     | 13344/29250 [06:14<05:04, 52.31it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▌     | 13344/29250 [06:15<05:04, 52.31it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▌     | 13376/29250 [06:15<04:38, 56.90it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▌     | 13376/29250 [06:15<04:38, 56.90it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▌     | 13408/29250 [06:15<03:55, 67.36it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▌     | 13408/29250 [06:15<03:55, 67.36it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▌     | 13440/29250 [06:15<04:09, 63.47it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▌     | 13440/29250 [06:16<04:09, 63.47it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▌     | 13472/29250 [06:16<04:13, 62.33it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▌     | 13472/29250 [06:16<04:13, 62.33it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▌     | 13504/29250 [06:16<03:43, 70.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▌     | 13504/29250 [06:17<03:43, 70.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▋     | 13536/29250 [06:17<04:27, 58.77it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▋     | 13536/29250 [06:17<04:27, 58.77it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▋     | 13568/29250 [06:17<03:27, 75.55it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▋     | 13568/29250 [06:18<03:27, 75.55it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▋     | 13600/29250 [06:18<03:12, 81.41it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  46%|████▋     | 13600/29250 [06:18<03:12, 81.41it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  47%|████▋     | 13632/29250 [06:18<04:02, 64.37it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  47%|████▋     | 13632/29250 [06:19<04:02, 64.37it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  47%|████▋     | 13664/29250 [06:19<03:59, 64.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  47%|████▋     | 13664/29250 [06:19<03:59, 64.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  47%|████▋     | 13696/29250 [06:19<04:03, 63.97it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  47%|████▋     | 13696/29250 [06:20<04:03, 63.97it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  47%|████▋     | 13728/29250 [06:20<03:29, 73.93it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  47%|████▋     | 13728/29250 [06:20<03:29, 73.93it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  47%|████▋     | 13760/29250 [06:20<03:11, 80.77it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  47%|████▋     | 13760/29250 [06:20<03:11, 80.77it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  47%|████▋     | 13792/29250 [06:20<03:49, 67.26it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  47%|████▋     | 13792/29250 [06:22<03:49, 67.26it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  47%|████▋     | 13824/29250 [06:22<06:19, 40.64it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  47%|████▋     | 13824/29250 [06:23<06:19, 40.64it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  47%|████▋     | 13856/29250 [06:23<05:40, 45.25it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  47%|████▋     | 13856/29250 [06:23<05:40, 45.25it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  47%|████▋     | 13888/29250 [06:23<05:24, 47.40it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  47%|████▋     | 13888/29250 [06:24<05:24, 47.40it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  48%|████▊     | 13920/29250 [06:24<05:11, 49.14it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  48%|████▊     | 13920/29250 [06:25<05:11, 49.14it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  48%|████▊     | 13952/29250 [06:25<06:31, 39.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  48%|████▊     | 13952/29250 [06:26<06:31, 39.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  48%|████▊     | 13984/29250 [06:26<06:20, 40.10it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  48%|████▊     | 13984/29250 [06:26<06:20, 40.10it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  48%|████▊     | 14016/29250 [06:27<07:06, 35.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  48%|████▊     | 14016/29250 [06:27<07:06, 35.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  48%|████▊     | 14048/29250 [06:27<05:43, 44.21it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  48%|████▊     | 14048/29250 [06:27<05:43, 44.21it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  48%|████▊     | 14080/29250 [06:27<04:50, 52.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  48%|████▊     | 14080/29250 [06:30<04:50, 52.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  48%|████▊     | 14112/29250 [06:30<09:59, 25.26it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  48%|████▊     | 14112/29250 [06:31<09:59, 25.26it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  48%|████▊     | 14144/29250 [06:31<07:49, 32.16it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  48%|████▊     | 14144/29250 [06:32<07:49, 32.16it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  48%|████▊     | 14176/29250 [06:32<08:32, 29.41it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  48%|████▊     | 14176/29250 [06:32<08:32, 29.41it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  49%|████▊     | 14208/29250 [06:32<06:29, 38.64it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  49%|████▊     | 14208/29250 [06:33<06:29, 38.64it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  49%|████▊     | 14240/29250 [06:33<05:27, 45.80it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  49%|████▊     | 14240/29250 [06:33<05:27, 45.80it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  49%|████▉     | 14272/29250 [06:33<05:09, 48.44it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  49%|████▉     | 14272/29250 [06:33<05:09, 48.44it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  49%|████▉     | 14304/29250 [06:33<04:00, 62.21it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  49%|████▉     | 14304/29250 [06:34<04:00, 62.21it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  49%|████▉     | 14336/29250 [06:34<05:14, 47.43it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  49%|████▉     | 14336/29250 [06:35<05:14, 47.43it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  49%|████▉     | 14368/29250 [06:35<04:39, 53.24it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  49%|████▉     | 14368/29250 [06:36<04:39, 53.24it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  49%|████▉     | 14400/29250 [06:36<05:10, 47.84it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  49%|████▉     | 14400/29250 [06:38<05:10, 47.84it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  49%|████▉     | 14432/29250 [06:38<08:57, 27.58it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  49%|████▉     | 14432/29250 [06:38<08:57, 27.58it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  49%|████▉     | 14464/29250 [06:38<06:48, 36.16it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  49%|████▉     | 14464/29250 [06:39<06:48, 36.16it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  50%|████▉     | 14496/29250 [06:39<05:39, 43.50it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  50%|████▉     | 14496/29250 [06:39<05:39, 43.50it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  50%|████▉     | 14528/29250 [06:39<04:49, 50.90it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  50%|████▉     | 14528/29250 [06:41<04:49, 50.90it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  50%|████▉     | 14560/29250 [06:41<08:05, 30.26it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  50%|████▉     | 14560/29250 [06:43<08:05, 30.26it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  50%|████▉     | 14592/29250 [06:43<09:23, 25.99it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  50%|████▉     | 14592/29250 [06:43<09:23, 25.99it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  50%|████▉     | 14624/29250 [06:43<07:20, 33.23it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  50%|████▉     | 14624/29250 [06:43<07:20, 33.23it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  50%|█████     | 14656/29250 [06:43<06:17, 38.65it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  50%|█████     | 14656/29250 [06:45<06:17, 38.65it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  50%|█████     | 14688/29250 [06:45<07:07, 34.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  50%|█████     | 14688/29250 [06:45<07:07, 34.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  50%|█████     | 14720/29250 [06:45<05:55, 40.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  50%|█████     | 14720/29250 [06:46<05:55, 40.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  50%|█████     | 14752/29250 [06:46<05:41, 42.51it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  50%|█████     | 14752/29250 [06:46<05:41, 42.51it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  51%|█████     | 14784/29250 [06:46<04:30, 53.52it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  51%|█████     | 14784/29250 [06:46<04:30, 53.52it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  51%|█████     | 14816/29250 [06:46<04:02, 59.55it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  51%|█████     | 14816/29250 [06:47<04:02, 59.55it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  51%|█████     | 14848/29250 [06:47<04:21, 55.01it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  51%|█████     | 14848/29250 [06:48<04:21, 55.01it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  51%|█████     | 14880/29250 [06:48<04:28, 53.57it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  51%|█████     | 14880/29250 [06:48<04:28, 53.57it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  51%|█████     | 14912/29250 [06:48<04:46, 50.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  51%|█████     | 14912/29250 [06:49<04:46, 50.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  51%|█████     | 14944/29250 [06:49<04:13, 56.52it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  51%|█████     | 14944/29250 [06:49<04:13, 56.52it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  51%|█████     | 14976/29250 [06:49<03:44, 63.70it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  51%|█████     | 14976/29250 [06:50<03:44, 63.70it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  51%|█████▏    | 15008/29250 [06:50<03:24, 69.72it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  51%|█████▏    | 15008/29250 [06:50<03:24, 69.72it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  51%|█████▏    | 15040/29250 [06:50<04:07, 57.44it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  51%|█████▏    | 15040/29250 [06:51<04:07, 57.44it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  52%|█████▏    | 15072/29250 [06:51<03:45, 62.86it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  52%|█████▏    | 15072/29250 [06:51<03:45, 62.86it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  52%|█████▏    | 15104/29250 [06:51<03:46, 62.56it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  52%|█████▏    | 15104/29250 [06:52<03:46, 62.56it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  52%|█████▏    | 15136/29250 [06:52<03:44, 62.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  52%|█████▏    | 15136/29250 [06:53<03:44, 62.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  52%|█████▏    | 15168/29250 [06:53<06:02, 38.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  52%|█████▏    | 15168/29250 [06:54<06:02, 38.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  52%|█████▏    | 15200/29250 [06:54<05:03, 46.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  52%|█████▏    | 15200/29250 [06:56<05:03, 46.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  52%|█████▏    | 15232/29250 [06:56<07:50, 29.79it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  52%|█████▏    | 15232/29250 [06:57<07:50, 29.79it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  52%|█████▏    | 15264/29250 [06:57<07:39, 30.43it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  52%|█████▏    | 15264/29250 [06:57<07:39, 30.43it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  52%|█████▏    | 15296/29250 [06:57<06:10, 37.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  52%|█████▏    | 15296/29250 [06:58<06:10, 37.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  52%|█████▏    | 15328/29250 [06:58<06:45, 34.35it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  52%|█████▏    | 15328/29250 [06:59<06:45, 34.35it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15360/29250 [06:59<05:50, 39.65it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15360/29250 [07:01<05:50, 39.65it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15392/29250 [07:01<09:52, 23.38it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15392/29250 [07:02<09:52, 23.38it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15424/29250 [07:02<07:39, 30.12it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15424/29250 [07:02<07:39, 30.12it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15456/29250 [07:02<06:02, 38.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15456/29250 [07:03<06:02, 38.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15488/29250 [07:03<06:34, 34.88it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15488/29250 [07:05<06:34, 34.88it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15520/29250 [07:05<07:51, 29.10it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15520/29250 [07:05<07:51, 29.10it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15552/29250 [07:05<07:13, 31.57it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15552/29250 [07:06<07:13, 31.57it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15584/29250 [07:06<06:21, 35.87it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15584/29250 [07:06<06:21, 35.87it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15616/29250 [07:06<05:02, 45.06it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15616/29250 [07:08<05:02, 45.06it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15648/29250 [07:08<06:55, 32.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  53%|█████▎    | 15648/29250 [07:09<06:55, 32.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  54%|█████▎    | 15680/29250 [07:09<05:58, 37.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  54%|█████▎    | 15680/29250 [07:09<05:58, 37.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  54%|█████▎    | 15712/29250 [07:09<04:27, 50.62it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  54%|█████▎    | 15712/29250 [07:09<04:27, 50.62it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  54%|█████▍    | 15744/29250 [07:09<03:53, 57.84it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  54%|█████▍    | 15744/29250 [07:10<03:53, 57.84it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  54%|█████▍    | 15776/29250 [07:10<03:55, 57.22it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  54%|█████▍    | 15776/29250 [07:11<03:55, 57.22it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  54%|█████▍    | 15808/29250 [07:11<05:23, 41.59it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  54%|█████▍    | 15808/29250 [07:12<05:23, 41.59it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  54%|█████▍    | 15840/29250 [07:12<05:28, 40.80it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  54%|█████▍    | 15840/29250 [07:12<05:28, 40.80it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  54%|█████▍    | 15872/29250 [07:12<04:18, 51.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  54%|█████▍    | 15872/29250 [07:14<04:18, 51.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  54%|█████▍    | 15904/29250 [07:14<06:21, 34.98it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  54%|█████▍    | 15904/29250 [07:14<06:21, 34.98it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  54%|█████▍    | 15936/29250 [07:14<05:01, 44.12it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  54%|█████▍    | 15936/29250 [07:15<05:01, 44.12it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  55%|█████▍    | 15968/29250 [07:15<06:21, 34.82it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  55%|█████▍    | 15968/29250 [07:16<06:21, 34.82it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  55%|█████▍    | 16000/29250 [07:16<06:22, 34.61it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  55%|█████▍    | 16000/29250 [07:17<06:22, 34.61it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  55%|█████▍    | 16032/29250 [07:17<05:57, 36.99it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  55%|█████▍    | 16032/29250 [07:21<05:57, 36.99it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  55%|█████▍    | 16064/29250 [07:21<11:48, 18.60it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  55%|█████▍    | 16064/29250 [07:21<11:48, 18.60it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  55%|█████▌    | 16096/29250 [07:21<09:29, 23.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  55%|█████▌    | 16096/29250 [07:22<09:29, 23.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  55%|█████▌    | 16128/29250 [07:22<07:41, 28.46it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  55%|█████▌    | 16128/29250 [07:22<07:41, 28.46it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  55%|█████▌    | 16160/29250 [07:22<06:23, 34.17it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  55%|█████▌    | 16160/29250 [07:23<06:23, 34.17it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  55%|█████▌    | 16192/29250 [07:23<05:27, 39.92it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  55%|█████▌    | 16192/29250 [07:24<05:27, 39.92it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  55%|█████▌    | 16224/29250 [07:24<05:35, 38.79it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  55%|█████▌    | 16224/29250 [07:25<05:35, 38.79it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  56%|█████▌    | 16256/29250 [07:25<06:19, 34.24it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  56%|█████▌    | 16256/29250 [07:25<06:19, 34.24it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  56%|█████▌    | 16288/29250 [07:25<04:56, 43.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  56%|█████▌    | 16288/29250 [07:26<04:56, 43.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  56%|█████▌    | 16320/29250 [07:26<05:56, 36.22it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  56%|█████▌    | 16320/29250 [07:28<05:56, 36.22it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  56%|█████▌    | 16352/29250 [07:28<07:57, 26.99it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  56%|█████▌    | 16352/29250 [07:28<07:57, 26.99it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  56%|█████▌    | 16384/29250 [07:28<06:19, 33.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  56%|█████▌    | 16384/29250 [07:29<06:19, 33.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  56%|█████▌    | 16416/29250 [07:29<05:35, 38.25it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  56%|█████▌    | 16416/29250 [07:29<05:35, 38.25it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  56%|█████▌    | 16448/29250 [07:29<04:36, 46.38it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  56%|█████▌    | 16448/29250 [07:30<04:36, 46.38it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  56%|█████▋    | 16480/29250 [07:30<03:56, 54.06it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  56%|█████▋    | 16480/29250 [07:30<03:56, 54.06it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  56%|█████▋    | 16512/29250 [07:30<03:51, 55.13it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  56%|█████▋    | 16512/29250 [07:31<03:51, 55.13it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  57%|█████▋    | 16544/29250 [07:31<03:25, 61.72it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  57%|█████▋    | 16544/29250 [07:33<03:25, 61.72it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  57%|█████▋    | 16576/29250 [07:33<06:34, 32.13it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  57%|█████▋    | 16576/29250 [07:33<06:34, 32.13it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  57%|█████▋    | 16608/29250 [07:33<05:11, 40.52it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  57%|█████▋    | 16608/29250 [07:34<05:11, 40.52it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  57%|█████▋    | 16640/29250 [07:34<04:33, 46.10it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  57%|█████▋    | 16640/29250 [07:34<04:33, 46.10it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  57%|█████▋    | 16672/29250 [07:34<04:08, 50.53it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  57%|█████▋    | 16672/29250 [07:34<04:08, 50.53it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  57%|█████▋    | 16704/29250 [07:34<03:39, 57.09it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  57%|█████▋    | 16704/29250 [07:35<03:39, 57.09it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  57%|█████▋    | 16736/29250 [07:35<03:29, 59.70it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  57%|█████▋    | 16736/29250 [07:35<03:29, 59.70it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  57%|█████▋    | 16768/29250 [07:35<02:47, 74.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  57%|█████▋    | 16768/29250 [07:35<02:47, 74.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  57%|█████▋    | 16800/29250 [07:35<02:23, 86.62it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  57%|█████▋    | 16800/29250 [07:36<02:23, 86.62it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  58%|█████▊    | 16832/29250 [07:36<02:13, 93.09it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  58%|█████▊    | 16832/29250 [07:37<02:13, 93.09it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  58%|█████▊    | 16864/29250 [07:37<03:20, 61.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  58%|█████▊    | 16864/29250 [07:37<03:20, 61.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  58%|█████▊    | 16896/29250 [07:37<03:19, 61.98it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  58%|█████▊    | 16896/29250 [07:37<03:19, 61.98it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  58%|█████▊    | 16928/29250 [07:37<02:40, 76.77it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  58%|█████▊    | 16928/29250 [07:38<02:40, 76.77it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  58%|█████▊    | 16960/29250 [07:38<02:27, 83.13it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  58%|█████▊    | 16960/29250 [07:40<02:27, 83.13it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  58%|█████▊    | 16992/29250 [07:40<07:01, 29.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  58%|█████▊    | 16992/29250 [07:42<07:01, 29.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  58%|█████▊    | 17024/29250 [07:42<08:58, 22.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  58%|█████▊    | 17024/29250 [07:43<08:58, 22.71it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  58%|█████▊    | 17056/29250 [07:43<07:34, 26.84it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  58%|█████▊    | 17056/29250 [07:43<07:34, 26.84it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  58%|█████▊    | 17088/29250 [07:43<05:41, 35.66it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  58%|█████▊    | 17088/29250 [07:44<05:41, 35.66it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  59%|█████▊    | 17120/29250 [07:44<05:35, 36.15it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  59%|█████▊    | 17120/29250 [07:45<05:35, 36.15it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  59%|█████▊    | 17152/29250 [07:45<05:43, 35.22it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  59%|█████▊    | 17152/29250 [07:46<05:43, 35.22it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  59%|█████▊    | 17184/29250 [07:46<05:08, 39.09it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  59%|█████▊    | 17184/29250 [07:49<05:08, 39.09it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  59%|█████▉    | 17216/29250 [07:49<08:49, 22.73it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  59%|█████▉    | 17216/29250 [07:49<08:49, 22.73it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  59%|█████▉    | 17248/29250 [07:49<06:54, 28.93it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  59%|█████▉    | 17248/29250 [07:49<06:54, 28.93it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  59%|█████▉    | 17280/29250 [07:49<05:32, 35.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  59%|█████▉    | 17280/29250 [07:50<05:32, 35.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  59%|█████▉    | 17312/29250 [07:50<04:25, 44.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  59%|█████▉    | 17312/29250 [07:50<04:25, 44.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  59%|█████▉    | 17344/29250 [07:50<03:34, 55.48it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  59%|█████▉    | 17344/29250 [07:50<03:34, 55.48it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  59%|█████▉    | 17376/29250 [07:50<02:50, 69.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  59%|█████▉    | 17376/29250 [07:50<02:50, 69.75it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|█████▉    | 17408/29250 [07:50<02:27, 80.09it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|█████▉    | 17408/29250 [07:51<02:27, 80.09it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|█████▉    | 17440/29250 [07:51<03:43, 52.87it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|█████▉    | 17440/29250 [07:55<03:43, 52.87it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|█████▉    | 17472/29250 [07:55<08:31, 23.04it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|█████▉    | 17472/29250 [07:55<08:31, 23.04it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|█████▉    | 17504/29250 [07:55<07:04, 27.68it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|█████▉    | 17504/29250 [07:56<07:04, 27.68it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|█████▉    | 17536/29250 [07:56<05:44, 33.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|█████▉    | 17536/29250 [07:58<05:44, 33.96it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|██████    | 17568/29250 [07:58<08:04, 24.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|██████    | 17568/29250 [08:00<08:04, 24.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|██████    | 17600/29250 [08:00<08:43, 22.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|██████    | 17600/29250 [08:00<08:43, 22.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|██████    | 17632/29250 [08:00<06:45, 28.67it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|██████    | 17632/29250 [08:01<06:45, 28.67it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|██████    | 17664/29250 [08:01<05:58, 32.35it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|██████    | 17664/29250 [08:01<05:58, 32.35it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|██████    | 17696/29250 [08:01<05:28, 35.22it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  60%|██████    | 17696/29250 [08:02<05:28, 35.22it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  61%|██████    | 17728/29250 [08:02<04:16, 44.86it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  61%|██████    | 17728/29250 [08:03<04:16, 44.86it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  61%|██████    | 17760/29250 [08:03<04:31, 42.32it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  61%|██████    | 17760/29250 [08:03<04:31, 42.32it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  61%|██████    | 17792/29250 [08:03<04:07, 46.21it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  61%|██████    | 17792/29250 [08:04<04:07, 46.21it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  61%|██████    | 17824/29250 [08:04<05:08, 37.08it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  61%|██████    | 17824/29250 [08:05<05:08, 37.08it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  61%|██████    | 17856/29250 [08:05<04:43, 40.13it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  61%|██████    | 17856/29250 [08:08<04:43, 40.13it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  61%|██████    | 17888/29250 [08:08<08:29, 22.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  61%|██████    | 17888/29250 [08:08<08:29, 22.30it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  61%|██████▏   | 17920/29250 [08:08<06:33, 28.82it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  61%|██████▏   | 17920/29250 [08:08<06:33, 28.82it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  61%|██████▏   | 17952/29250 [08:08<04:54, 38.34it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  61%|██████▏   | 17952/29250 [08:09<04:54, 38.34it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  61%|██████▏   | 17984/29250 [08:09<04:37, 40.53it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  61%|██████▏   | 17984/29250 [08:10<04:37, 40.53it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  62%|██████▏   | 18016/29250 [08:10<04:28, 41.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  62%|██████▏   | 18016/29250 [08:10<04:28, 41.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  62%|██████▏   | 18048/29250 [08:10<04:07, 45.17it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  62%|██████▏   | 18048/29250 [08:12<04:07, 45.17it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  62%|██████▏   | 18080/29250 [08:12<05:17, 35.17it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  62%|██████▏   | 18080/29250 [08:12<05:17, 35.17it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  62%|██████▏   | 18112/29250 [08:12<04:03, 45.70it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  62%|██████▏   | 18112/29250 [08:13<04:03, 45.70it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  62%|██████▏   | 18144/29250 [08:13<04:00, 46.14it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  62%|██████▏   | 18144/29250 [08:13<04:00, 46.14it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  62%|██████▏   | 18176/29250 [08:13<03:03, 60.24it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  62%|██████▏   | 18176/29250 [08:14<03:03, 60.24it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  62%|██████▏   | 18208/29250 [08:14<03:25, 53.79it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  62%|██████▏   | 18208/29250 [08:15<03:25, 53.79it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  62%|██████▏   | 18240/29250 [08:15<04:51, 37.73it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  62%|██████▏   | 18240/29250 [08:15<04:51, 37.73it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  62%|██████▏   | 18272/29250 [08:15<04:01, 45.43it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  62%|██████▏   | 18272/29250 [08:16<04:01, 45.43it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  63%|██████▎   | 18304/29250 [08:16<03:05, 59.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  63%|██████▎   | 18304/29250 [08:16<03:05, 59.07it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  63%|██████▎   | 18336/29250 [08:16<03:33, 51.04it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  63%|██████▎   | 18336/29250 [08:17<03:33, 51.04it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  63%|██████▎   | 18368/29250 [08:17<02:45, 65.66it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  63%|██████▎   | 18368/29250 [08:17<02:45, 65.66it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  63%|██████▎   | 18400/29250 [08:17<02:46, 65.20it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  63%|██████▎   | 18400/29250 [08:18<02:46, 65.20it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  63%|██████▎   | 18432/29250 [08:18<03:07, 57.83it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  63%|██████▎   | 18432/29250 [08:19<03:07, 57.83it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  63%|██████▎   | 18464/29250 [08:19<03:46, 47.57it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  63%|██████▎   | 18464/29250 [08:19<03:46, 47.57it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  63%|██████▎   | 18496/29250 [08:19<03:14, 55.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  63%|██████▎   | 18496/29250 [08:21<03:14, 55.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  63%|██████▎   | 18528/29250 [08:21<05:40, 31.48it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  63%|██████▎   | 18528/29250 [08:22<05:40, 31.48it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  63%|██████▎   | 18560/29250 [08:22<04:42, 37.89it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  63%|██████▎   | 18560/29250 [08:22<04:42, 37.89it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  64%|██████▎   | 18592/29250 [08:22<04:20, 40.98it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  64%|██████▎   | 18592/29250 [08:22<04:20, 40.98it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  64%|██████▎   | 18624/29250 [08:22<03:31, 50.36it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  64%|██████▎   | 18624/29250 [08:23<03:31, 50.36it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  64%|██████▍   | 18656/29250 [08:23<02:49, 62.62it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  64%|██████▍   | 18656/29250 [08:23<02:49, 62.62it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  64%|██████▍   | 18688/29250 [08:23<02:28, 70.93it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  64%|██████▍   | 18688/29250 [08:24<02:28, 70.93it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  64%|██████▍   | 18720/29250 [08:24<02:56, 59.73it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  64%|██████▍   | 18720/29250 [08:24<02:56, 59.73it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  64%|██████▍   | 18752/29250 [08:24<03:08, 55.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  64%|██████▍   | 18752/29250 [08:25<03:08, 55.69it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  64%|██████▍   | 18784/29250 [08:25<03:22, 51.81it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  64%|██████▍   | 18784/29250 [08:26<03:22, 51.81it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  64%|██████▍   | 18816/29250 [08:26<03:06, 55.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  64%|██████▍   | 18816/29250 [08:27<03:06, 55.85it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  64%|██████▍   | 18848/29250 [08:27<04:28, 38.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  64%|██████▍   | 18848/29250 [08:27<04:28, 38.74it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  65%|██████▍   | 18880/29250 [08:27<03:41, 46.79it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  65%|██████▍   | 18880/29250 [08:28<03:41, 46.79it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  65%|██████▍   | 18912/29250 [08:28<04:04, 42.29it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  65%|██████▍   | 18912/29250 [08:28<04:04, 42.29it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  65%|██████▍   | 18944/29250 [08:29<03:10, 54.02it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  65%|██████▍   | 18944/29250 [08:29<03:10, 54.02it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  65%|██████▍   | 18976/29250 [08:29<02:59, 57.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  65%|██████▍   | 18976/29250 [08:30<02:59, 57.27it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  65%|██████▍   | 19008/29250 [08:30<02:56, 58.16it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  65%|██████▍   | 19008/29250 [08:30<02:56, 58.16it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  65%|██████▌   | 19040/29250 [08:30<02:21, 72.02it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  65%|██████▌   | 19040/29250 [08:30<02:21, 72.02it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  65%|██████▌   | 19072/29250 [08:30<02:32, 66.58it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  65%|██████▌   | 19072/29250 [08:30<02:32, 66.58it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  65%|██████▌   | 19104/29250 [08:30<02:04, 81.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  65%|██████▌   | 19104/29250 [08:31<02:04, 81.42it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  65%|██████▌   | 19136/29250 [08:31<02:02, 82.61it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  65%|██████▌   | 19136/29250 [08:31<02:02, 82.61it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  66%|██████▌   | 19168/29250 [08:31<02:07, 78.98it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  66%|██████▌   | 19168/29250 [08:33<02:07, 78.98it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  66%|██████▌   | 19200/29250 [08:33<03:44, 44.73it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  66%|██████▌   | 19200/29250 [08:34<03:44, 44.73it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  66%|██████▌   | 19232/29250 [08:34<04:24, 37.87it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  66%|██████▌   | 19232/29250 [08:35<04:24, 37.87it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  66%|██████▌   | 19264/29250 [08:35<04:22, 38.02it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  66%|██████▌   | 19264/29250 [08:36<04:22, 38.02it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  66%|██████▌   | 19296/29250 [08:36<04:45, 34.88it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  66%|██████▌   | 19296/29250 [08:36<04:45, 34.88it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  66%|██████▌   | 19328/29250 [08:36<03:44, 44.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  66%|██████▌   | 19328/29250 [08:36<03:44, 44.11it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  66%|██████▌   | 19360/29250 [08:36<03:15, 50.62it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  66%|██████▌   | 19360/29250 [08:37<03:15, 50.62it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  66%|██████▋   | 19392/29250 [08:38<03:49, 42.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  66%|██████▋   | 19392/29250 [08:38<03:49, 42.91it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  66%|██████▋   | 19424/29250 [08:38<03:05, 52.94it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  66%|██████▋   | 19424/29250 [08:38<03:05, 52.94it/s]\u001b[A\n",
      "\u001b[36mTraining\u001b[0m - Epochs: 001/001:  67%|██████▋   | 19456/29250 [08:38<02:23, 68.15it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    {\"train\": train_loader,\n",
    "     \"val\": val_loader},\n",
    "    epochs = 1,\n",
    "    checkpoint_path=\"/search-data/evan/phenotype_model/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84886975-d404-4af5-a986-83b58d4060f5",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16277af-be02-4414-9f22-f8f69dd5603b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
    "test_result = evaluate_model(model.model, test_loader, model.criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcbd4a2-a613-4dd2-a42f-2012e723f0b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.run.log({\n",
    "    'test_AUROC_micro' : test_result['AUROC_micro'],\n",
    "    \"test_AUROC_macro\" : test_result['AUROC_macro'],\n",
    "    'test_AUROC_weighted' : test_result['AUROC_weighted']\n",
    "})\n",
    "model.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a28efb-0878-4d2b-b745-3c15f2e91731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_to_file(\"/search-data/evan/phenotype_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f888afe6-3df5-4c47-b475-198a70dab28e",
   "metadata": {},
   "source": [
    "# Save & Load (Run Separately) - To Skip Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e99acda-2e81-4db9-bb03-2b3ff747fb46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"/search-data/evan/data/phenotyping/\" # input Your Data Dir Here Pointing To /in-hospital-mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bab3085f-0375-4307-a301-9478afb4eaca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_x = torch.load(f\"{data_dir}/phenotype_train_x.pt\", mmap=True)\n",
    "train_y = torch.load(f\"{data_dir}/phenotype_train_y.pt\", mmap=True)\n",
    "train_mask = torch.load(f\"{data_dir}/phenotype_train_mask.pt\", mmap=True)\n",
    "val_x   = torch.load(f\"{data_dir}/phenotype_val_x.pt\", mmap=True)\n",
    "val_y   = torch.load(f\"{data_dir}/phenotype_val_y.pt\", mmap=True)\n",
    "val_mask   = torch.load(f\"{data_dir}/phenotype_val_mask.pt\", mmap=True)\n",
    "test_x   = torch.load(f\"{data_dir}/phenotype_test_x.pt\", mmap=True)\n",
    "test_y   = torch.load(f\"{data_dir}/phenotype_test_y.pt\", mmap=True)\n",
    "test_mask   = torch.load(f\"{data_dir}/phenotype_test_mask.pt\", mmap=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "983760ae-3d43-461d-9984-5e2364f15a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_x, train_y, train_mask)\n",
    "val_ds = TensorDataset(val_x, val_y, val_mask)\n",
    "test_ds = TensorDataset(test_x, test_y, test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405abbfd-960c-4067-af37-2d4dddaa256f",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4983163-6467-4add-99c1-f91049c434ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▁▁▂▂▂▂▂▃▃▃▄▅▅▆▆▇▇██</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_AUROC_macro</td><td>▁▁▁▂▁▄▄▄▃▄▃▂▄▃▅▅▃▄█▆</td></tr><tr><td>val_AUROC_micro</td><td>▁▅▆▇▇▇█▅▆▇▅▃▃▅▇▃▄██▇</td></tr><tr><td>val_AUROC_weighted</td><td>▁▂▃▃▄▅▅▄▃▄▃▁▃▄▅▅▃▆█▆</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.31682</td></tr><tr><td>train_loss</td><td>2.44181</td></tr><tr><td>val_AUROC_macro</td><td>0.60217</td></tr><tr><td>val_AUROC_micro</td><td>0.73259</td></tr><tr><td>val_AUROC_weighted</td><td>0.60625</td></tr><tr><td>val_loss</td><td>286.17506</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">soft-bee-66</strong> at: <a href='https://wandb.ai/millikanevan-personal/sand-mimic3/runs/7y3f3mbb' target=\"_blank\">https://wandb.ai/millikanevan-personal/sand-mimic3/runs/7y3f3mbb</a><br> View project at: <a href='https://wandb.ai/millikanevan-personal/sand-mimic3' target=\"_blank\">https://wandb.ai/millikanevan-personal/sand-mimic3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251116_112903-7y3f3mbb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/mimic3-sand/wandb/run-20251116_133542-hhv23r5h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/millikanevan-personal/sand-mimic3/runs/hhv23r5h' target=\"_blank\">phenotyping-baseline</a></strong> to <a href='https://wandb.ai/millikanevan-personal/sand-mimic3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/millikanevan-personal/sand-mimic3' target=\"_blank\">https://wandb.ai/millikanevan-personal/sand-mimic3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/millikanevan-personal/sand-mimic3/runs/hhv23r5h' target=\"_blank\">https://wandb.ai/millikanevan-personal/sand-mimic3/runs/hhv23r5h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_AUROC_macro</td><td>▁</td></tr><tr><td>test_AUROC_micro</td><td>▁</td></tr><tr><td>test_AUROC_weighted</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_AUROC_macro</td><td>0.766</td></tr><tr><td>test_AUROC_micro</td><td>0.816</td></tr><tr><td>test_AUROC_weighted</td><td>0.754</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">phenotyping-baseline</strong> at: <a href='https://wandb.ai/millikanevan-personal/sand-mimic3/runs/hhv23r5h' target=\"_blank\">https://wandb.ai/millikanevan-personal/sand-mimic3/runs/hhv23r5h</a><br> View project at: <a href='https://wandb.ai/millikanevan-personal/sand-mimic3' target=\"_blank\">https://wandb.ai/millikanevan-personal/sand-mimic3</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251116_133542-hhv23r5h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project='sand-mimic3',\n",
    "    config={\n",
    "        \"task\" : \"phenotype\",\n",
    "    },\n",
    "    name='phenotyping-baseline'\n",
    ")\n",
    "run.log({\n",
    "    'test_AUROC_micro' : 0.816,\n",
    "    \"test_AUROC_macro\" : 0.766,\n",
    "    'test_AUROC_weighted' : 0.754\n",
    "})\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e3a56-890e-4445-bba1-18321915c2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic3-sand",
   "language": "python",
   "name": "mimic3-sand"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
