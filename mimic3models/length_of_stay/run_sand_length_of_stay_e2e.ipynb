{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b32576-8426-4a8c-9910-07e0ad76cecd",
   "metadata": {},
   "source": [
    "# SAnD model predictions for length of Stay: Ablation study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc214675-6011-4ccd-8960-78dc429c3d39",
   "metadata": {},
   "source": [
    "### 0. Read in libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ce6152-2484-4dd3-b29f-d5f79dacc3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current File Path: /home/jovyan/mimic3-sand/mimic3models/length_of_stay\n",
      "Base Path: /home/jovyan/mimic3-sand\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, mean_absolute_error, mean_squared_error\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "import wandb\n",
    "\n",
    "cur_path = Path(\".\").resolve()\n",
    "base_path = cur_path.parents[1]\n",
    "\n",
    "print(f\"Current File Path: {cur_path}\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "\n",
    "os.chdir(str(base_path))\n",
    "sys.path.append(str(base_path))\n",
    "\n",
    "# Local modules\n",
    "from mimic3benchmark.readers import LengthOfStayReader\n",
    "from mimic3models import common_utils, metrics\n",
    "from mimic3models.length_of_stay import utils\n",
    "from mimic3models.preprocessing import Discretizer, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbedd523-9d91-4f4c-9eb9-7887e4975765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379fd13-cae6-4c3e-9123-10962d949930",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd() + '/data/length_of_stay'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aa1236-5ea8-4cc0-a924-812ec7edb125",
   "metadata": {},
   "source": [
    "### 1. Define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "088aee3c-fd26-4005-9b5d-135611eab593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes:\n",
    "# They use 25 Chunks of 20k rows each. We use 20 Chunks of 5k rows each.\n",
    "# I also dont understand the predicting per timestep part.\n",
    "# I tried loading data using deep supervision and the amount of data is substantially less only 29k rows.\n",
    "# I guess by loading normally, we already are using the format of predicting the data per timestep\n",
    "\n",
    "# Define model parameters\n",
    "in_feature = 76\n",
    "seq_len = 300\n",
    "n_heads = 8 # Number of heads for multi-head attention layer: Should be fixed at 8\n",
    "factor = 12 # Dense interpolation factor (M): This depends on the task at hand\n",
    "num_class = 10 # Number of output class\n",
    "num_layers = 3 # Number of multi-head attention layers (N): This depends on the task at hand\n",
    "d_model = 256 # Original 256\n",
    "dropout_rate = 0.4\n",
    "lookback = 96\n",
    "optimizer_config = {\n",
    "    'lr' : 0.001,\n",
    "    'betas' : (0.9, 0.98),\n",
    "    'eps' : 1e-08,\n",
    "}\n",
    "num_epochs = 1\n",
    "batch_size = 128 # Original 128\n",
    "\n",
    "num_chunks = 60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9e4f4e-0b50-4101-bb6c-83d1727323ac",
   "metadata": {},
   "source": [
    "### 2. Define model architecture for length of stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afab52cb-3712-4329-8077-291c42825c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, seq_len) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "\n",
    "        for pos in range(seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
    "                pe[pos, i+1] = math.cos(pos / (10000 ** ((2 * (i+1)) / d_model)))\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        seq_len = x.shape[1]\n",
    "        x = math.sqrt(self.d_model) * x\n",
    "        x = x + self.pe[:, :seq_len].requires_grad_(False)\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, layer: nn.Module, embed_dim: int, seq_len, lookback=None, p=0.1) -> None:\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.layer = layer\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.attn_weights = None\n",
    "        self.seq_len = seq_len\n",
    "        self.lookback = lookback\n",
    "\n",
    "        # Build Attention Window Causal Mask\n",
    "        with torch.no_grad():\n",
    "            idx = torch.arange(seq_len)\n",
    "            i = idx.unsqueeze(1)\n",
    "            j = idx.unsqueeze(0)\n",
    "            \n",
    "            future_mask = j > i\n",
    "            if lookback is None:\n",
    "                invalid_attention_mask = torch.zeros_like(future_mask)\n",
    "            else:\n",
    "                invalid_attention_mask = j < (i - lookback)\n",
    "            \n",
    "            # Set mask to 1 for all values that we shouldn't be paying attention to\n",
    "            combined_mask = future_mask | invalid_attention_mask\n",
    "            combined_mask = combined_mask.to(torch.bool)\n",
    "            \n",
    "        self.register_buffer(\"attn_mask\", combined_mask)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, pad_mask = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param x: [N, seq_len, features]\n",
    "        :return: [N, seq_len, features]\n",
    "        \"\"\"\n",
    "        if isinstance(self.layer, nn.MultiheadAttention):\n",
    "            src = x.transpose(0, 1)     # [seq_len, N, features]\n",
    "            \n",
    "            # Combine the padding mask and lookback mask into a single attention mask.\n",
    "            # In some cases, a query may be completely masked out by the combination of lookback and padding,\n",
    "            # which would result in all -inf attention scores and produce NaNs after softmax.\n",
    "            # To prevent this, we always allow each query to attend to itself by unmasking the diagonal.\n",
    "            mask = self.attn_mask.to(device)\n",
    "            mask = mask.unsqueeze(0).expand(x.shape[0] * self.layer.num_heads, -1, -1)\n",
    "            pad_mask = pad_mask.repeat_interleave(self.layer.num_heads, dim=0)\n",
    "            final_mask = mask + pad_mask.unsqueeze(1)\n",
    "            diagonal = torch.arange(self.seq_len, device=device)\n",
    "            final_mask[:, diagonal, diagonal] = False\n",
    "\n",
    "            output, self.attn_weights = self.layer(src, src, src, attn_mask=final_mask)\n",
    "            output = output.transpose(0, 1)     # [N, seq_len, features]\n",
    "\n",
    "        else:\n",
    "            output = self.layer(x)\n",
    "\n",
    "        output = self.dropout(output)\n",
    "        output = self.norm(x + output)\n",
    "        return output\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size: int) -> None:\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(hidden_size, hidden_size * 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_size * 2, hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        tensor = tensor.transpose(1, 2)\n",
    "        tensor = self.conv(tensor)\n",
    "        tensor = tensor.transpose(1, 2)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_head: int, seq_len, lookback=None, dropout_rate=0.1) -> None:\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attention = ResidualBlock(\n",
    "            nn.MultiheadAttention(embed_dim, num_head), embed_dim, seq_len, lookback, p=dropout_rate\n",
    "        )\n",
    "        self.ffn = ResidualBlock(PositionWiseFeedForward(embed_dim), embed_dim, seq_len, lookback, p=dropout_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, pad_mask) -> torch.Tensor:\n",
    "        x = self.attention(x, pad_mask)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "    \n",
    "class DenseInterpolation(nn.Module):\n",
    "    def __init__(self, seq_len: int, factor: int) -> None:\n",
    "        \"\"\"\n",
    "        :param seq_len: sequence length\n",
    "        :param factor: factor M\n",
    "        \"\"\"\n",
    "        super(DenseInterpolation, self).__init__()\n",
    "\n",
    "        s = torch.linspace(factor / seq_len, factor, steps=seq_len)\n",
    "        m = torch.arange(1, factor + 1).unsqueeze(1)\n",
    "        \n",
    "        tmp = 1 - torch.abs(s - m) / factor\n",
    "        w = tmp.clamp(min=0).pow(2)\n",
    "\n",
    "        W = w.unsqueeze(0)\n",
    "        self.register_buffer(\"W\", W)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        w = self.W.expand(x.size(0), -1, -1)\n",
    "        u = torch.bmm(w, x)\n",
    "        return u.transpose(1, 2)\n",
    "\n",
    "class ClassificationModule(nn.Module):\n",
    "    def __init__(self, d_model: int, factor: int, num_class: int) -> None:\n",
    "        super(ClassificationModule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.factor = factor\n",
    "        self.num_class = num_class\n",
    "\n",
    "        self.fc = nn.Linear(int(d_model * factor), num_class)\n",
    "\n",
    "        nn.init.normal_(self.fc.weight, std=0.02)\n",
    "        nn.init.normal_(self.fc.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.contiguous().view(-1, int(self.factor * self.d_model))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class RegressionModule(nn.Module):\n",
    "    def __init__(self, d_model: int, factor: int, output_size: int) -> None:\n",
    "        super(RegressionModule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.factor = factor\n",
    "        self.output_size = output_size\n",
    "        self.fc = nn.Linear(int(d_model * factor), output_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.contiguous().view(-1, int(self.factor * self.d_model))\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2140e681-eafc-4027-87d4-56f21082ef41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLayerForSAnD(nn.Module):\n",
    "    def __init__(self, input_features, seq_len, n_heads, n_layers, d_model=128, lookback=None, dropout_rate=0.2) -> None:\n",
    "        super(EncoderLayerForSAnD, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.input_embedding = nn.Conv1d(input_features, d_model, 1)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, seq_len)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            EncoderBlock(d_model, n_heads, seq_len, lookback, dropout_rate) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, pad_mask) -> torch.Tensor:\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.input_embedding(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        for l in self.blocks:\n",
    "            x = l(x, pad_mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SAnD(nn.Module):\n",
    "    \"\"\"\n",
    "    Simply Attend and Diagnose model\n",
    "\n",
    "    The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)\n",
    "\n",
    "    `Attend and Diagnose: Clinical Time Series Analysis Using Attention Models <https://arxiv.org/abs/1711.03905>`_\n",
    "    Huan Song, Deepta Rajan, Jayaraman J. Thiagarajan, Andreas Spanias\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, input_features: int, seq_len: int, n_heads: int, factor: int,\n",
    "            n_class: int, n_layers: int, d_model: int = 128, lookback=None, dropout_rate: float = 0.2\n",
    "    ) -> None:\n",
    "        super(SAnD, self).__init__()\n",
    "        \n",
    "        self.hyperparams = {\n",
    "             \"input_features\" : input_features,\n",
    "            \"seq_len\" : seq_len,\n",
    "            \"n_heads\" : n_heads, \n",
    "            \"factor\" : factor,\n",
    "            \"n_class\" : n_class,\n",
    "            \"n_layers\" : n_layers,\n",
    "            \"d_model\" : d_model,\n",
    "            \"lookback\" : lookback,\n",
    "            \"dropout_rate\" : dropout_rate\n",
    "        }\n",
    "        \n",
    "        self.encoder = EncoderLayerForSAnD(input_features, seq_len, n_heads, n_layers, d_model, lookback, dropout_rate)\n",
    "        self.dense_interpolation = DenseInterpolation(seq_len, factor)\n",
    "        self.clf = ClassificationModule(d_model, factor, n_class)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, pad_mask) -> torch.Tensor:\n",
    "        x = self.encoder(x, pad_mask)\n",
    "        x = self.dense_interpolation(x)\n",
    "        x = self.clf(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e5dbef7-c9aa-4b6c-abb1-bbe56ed72fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / (y_true + 0.1))) * 100\n",
    "\n",
    "class CustomBins:\n",
    "    inf = 1e18\n",
    "    bins = [(-inf, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 14), (14, +inf)]\n",
    "    nbins = len(bins)\n",
    "    means = [11.450379, 35.070846, 59.206531, 83.382723, 107.487817,\n",
    "             131.579534, 155.643957, 179.660558, 254.306624, 585.325890]\n",
    "\n",
    "\n",
    "def get_bin_custom(x, nbins, one_hot=False):\n",
    "    for i in range(nbins):\n",
    "        a = CustomBins.bins[i][0] * 24.0\n",
    "        b = CustomBins.bins[i][1] * 24.0\n",
    "        if a <= x < b:\n",
    "            if one_hot:\n",
    "                ret = np.zeros((CustomBins.nbins,))\n",
    "                ret[i] = 1\n",
    "                return ret\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_estimate_custom(prediction, nbins):\n",
    "    bin_id = np.argmax(prediction)\n",
    "    assert 0 <= bin_id < nbins\n",
    "    return CustomBins.means[bin_id]\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device=device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_labels_binned = []\n",
    "    all_probs = []\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, labels_binned, pad_mask in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels_binned = labels_binned.to(device)\n",
    "            pad_mask = pad_mask.to(device)\n",
    "\n",
    "            logits = model(inputs, pad_mask)\n",
    "\n",
    "            loss = criterion(logits, labels_binned)\n",
    "            total_loss += loss.cpu().item()\n",
    "\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_labels_binned.append(labels_binned.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "\n",
    "    \n",
    "    y_true = torch.cat(all_labels).numpy()\n",
    "    y_true_binned = torch.cat(all_labels_binned).numpy()\n",
    "    y_pred = torch.cat(all_probs).numpy()\n",
    "    y_pred_binned = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Must turn y_pred into bins\n",
    "    kappa = cohen_kappa_score(y_true_binned, y_pred_binned, weights=\"linear\")\n",
    "    \n",
    "    y_pred_means = [get_estimate_custom(pred_prob, 10) for pred_prob in y_pred]\n",
    "    # regression metrics (same as print_metrics_regression)\n",
    "    mad = mean_absolute_error(y_true, y_pred_means)\n",
    "    mse = mean_squared_error(y_true, y_pred_means)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred_means)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss,\n",
    "        \"MAD\": mad,\n",
    "        \"MSE\": mse,\n",
    "        \"MAPE\": mape,\n",
    "        \"kappa\": kappa\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "887a1c9a-b565-4f26-999b-b15b44911d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, model, criterion, optimizer, optimizer_config: dict, experiment) -> None:\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = optimizer(self.model.parameters(), **optimizer_config)\n",
    "        self.criterion = criterion\n",
    "\n",
    "        self.hyper_params = optimizer_config\n",
    "        self._start_epoch = 0\n",
    "        self.hyper_params[\"epochs\"] = self._start_epoch\n",
    "        self.__num_classes = None\n",
    "        self._is_parallel = False\n",
    "        \n",
    "        self.run = wandb.init(\n",
    "            project='sand-mimic3',\n",
    "            config={\n",
    "                \"task\" : \"length_of_stay\",\n",
    "                \"hyperparams\" : self.model.hyperparams\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "            self._is_parallel = True\n",
    "\n",
    "            notice = \"Running on {} GPUs.\".format(torch.cuda.device_count())\n",
    "            print(\"\\033[33m\" + notice + \"\\033[0m\")            \n",
    "\n",
    "    def fit(self, loader: Dict[str, DataLoader], epochs: int, checkpoint_path: str = None, validation: bool = True) -> None:\n",
    "        len_of_train_dataset = len(loader[\"train\"].dataset)\n",
    "        epochs = epochs + self._start_epoch\n",
    "\n",
    "        self.hyper_params[\"epochs\"] = epochs\n",
    "        self.hyper_params[\"batch_size\"] = loader[\"train\"].batch_size\n",
    "        self.hyper_params[\"train_ds_size\"] = len_of_train_dataset\n",
    "        \n",
    "        best_model = None\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "        if validation:\n",
    "            len_of_val_dataset = len(loader[\"val\"].dataset)\n",
    "            self.hyper_params[\"val_ds_size\"] = len_of_val_dataset\n",
    "\n",
    "        for epoch in range(self._start_epoch, epochs):\n",
    "            if checkpoint_path is not None and epoch % 100 == 0:\n",
    "                self.save_to_file(checkpoint_path)\n",
    "\n",
    "            correct = 0.0\n",
    "            total = 0.0\n",
    "\n",
    "            self.model.train()\n",
    "            pbar = tqdm.tqdm(total=len_of_train_dataset)\n",
    "            total_loss = 0\n",
    "            for x, y_reg, y, pad_mask in loader[\"train\"]:\n",
    "                b_size = y.shape[0]\n",
    "                total += y.shape[0]\n",
    "                x = x.to(self.device) if isinstance(x, torch.Tensor) else [i.to(self.device) for i in x]\n",
    "                y = y.to(self.device)\n",
    "                pad_mask = pad_mask.to(self.device)\n",
    "\n",
    "                pbar.set_description(\n",
    "                    \"\\033[36m\" + \"Training\" + \"\\033[0m\" + \" - Epochs: {:03d}/{:03d}\".format(epoch+1, epochs)\n",
    "                )\n",
    "                pbar.update(b_size)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(x, pad_mask)\n",
    "                loss = self.criterion(outputs, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == y).sum().float().cpu().item()\n",
    "\n",
    "                total_loss += loss.cpu().item()\n",
    "\n",
    "            total_loss = total_loss / len(loader['train'])\n",
    "\n",
    "            if validation:\n",
    "                with torch.no_grad():\n",
    "                    self.model.eval()\n",
    "                    eval_result = evaluate_model(self.model, loader['val'], criterion=self.criterion)\n",
    "\n",
    "                    if eval_result['loss'] < best_val_loss:\n",
    "                        best_val_loss = eval_result['loss']\n",
    "                        state = self.model.module.state_dict() if self._is_parallel else self.model.state_dict()\n",
    "                        best_model = deepcopy(state)\n",
    "\n",
    "            if validation:\n",
    "                self.run.log({\n",
    "                    'train_loss' : total_loss, \n",
    "                    'train_accuracy' : float(correct / total), \n",
    "                    'val_loss' : eval_result['loss'], \n",
    "                    'val_kappa' : eval_result['kappa'],\n",
    "                    \"val_MSE\" : eval_result['MSE'],\n",
    "                    'val_MAPE' : eval_result['MAPE']\n",
    "                })\n",
    "            else:\n",
    "                self.run.log({\n",
    "                    'train_loss' : total_loss, \n",
    "                    'train_accuracy' : float(correct / total)\n",
    "                })\n",
    "\n",
    "            pbar.close()\n",
    "\n",
    "        if best_model is not None:\n",
    "            if self._is_parallel:\n",
    "                self.model.module.load_state_dict(best_model)\n",
    "            else:\n",
    "                self.model.load_state_dict(best_model)\n",
    "\n",
    "    def save_checkpoint(self) -> dict:\n",
    "        \"\"\"\n",
    "        The method of saving trained PyTorch model.\n",
    "\n",
    "        Note,  return value contains\n",
    "            - the number of last epoch as `epochs`\n",
    "            - optimizer state as `optimizer_state_dict`\n",
    "            - model state as `model_state_dict`\n",
    "\n",
    "        ::\n",
    "\n",
    "            clf = NeuralNetworkClassifier(\n",
    "                    Network(), nn.CrossEntropyLoss(),\n",
    "                    optim.Adam, optimizer_config, experiment\n",
    "                )\n",
    "\n",
    "            clf.fit(train_loader, epochs=10)\n",
    "            checkpoints = clf.save_checkpoint()\n",
    "\n",
    "        :return: dict {'epoch', 'optimizer_state_dict', 'model_state_dict'}\n",
    "        \"\"\"\n",
    "\n",
    "        checkpoints = {\n",
    "            \"epoch\": deepcopy(self.hyper_params[\"epochs\"]),\n",
    "            \"optimizer_state_dict\": deepcopy(self.optimizer.state_dict())\n",
    "        }\n",
    "\n",
    "        if self._is_parallel:\n",
    "            checkpoints[\"model_state_dict\"] = deepcopy(self.model.module.state_dict())\n",
    "        else:\n",
    "            checkpoints[\"model_state_dict\"] = deepcopy(self.model.state_dict())\n",
    "\n",
    "        return checkpoints\n",
    "\n",
    "    def save_to_file(self, path: str) -> str:\n",
    "        \"\"\"\n",
    "        | The method of saving trained PyTorch model to file.\n",
    "        | Those weights are uploaded to comet.ml as backup.\n",
    "        | check \"Asserts\".\n",
    "\n",
    "        Note, .pth file contains\n",
    "            - the number of last epoch as `epochs`\n",
    "            - optimizer state as `optimizer_state_dict`\n",
    "            - model state as `model_state_dict`\n",
    "\n",
    "        ::\n",
    "\n",
    "            clf = NeuralNetworkClassifier(\n",
    "                    Network(), nn.CrossEntropyLoss(),\n",
    "                    optim.Adam, optimizer_config, experiment\n",
    "                )\n",
    "\n",
    "            clf.fit(train_loader, epochs=10)\n",
    "            filename = clf.save_to_file('path/to/save/dir/')\n",
    "\n",
    "        :param path: path to saving directory. : string\n",
    "        :return: path to file : string\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(path):\n",
    "            os.mkdir(path)\n",
    "\n",
    "        file_name = \"model_params-epochs_{}-{}.pth\".format(\n",
    "            self.hyper_params[\"epochs\"], time.ctime().replace(\" \", \"_\")\n",
    "        )\n",
    "        path = path + file_name\n",
    "\n",
    "        checkpoints = self.save_checkpoint()\n",
    "\n",
    "        torch.save(checkpoints, path)\n",
    "\n",
    "        return path\n",
    "\n",
    "    def restore_checkpoint(self, checkpoints: dict) -> None:\n",
    "        \"\"\"\n",
    "        The method of loading trained PyTorch model.\n",
    "\n",
    "        :param checkpoints: dictionary which contains {'epoch', 'optimizer_state_dict', 'model_state_dict'}\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self._start_epoch = checkpoints[\"epoch\"]\n",
    "        if not isinstance(self._start_epoch, int):\n",
    "            raise TypeError\n",
    "\n",
    "        if self._is_parallel:\n",
    "            self.model.module.load_state_dict(checkpoints[\"model_state_dict\"])\n",
    "        else:\n",
    "            self.model.load_state_dict(checkpoints[\"model_state_dict\"])\n",
    "\n",
    "        self.optimizer.load_state_dict(checkpoints[\"optimizer_state_dict\"])\n",
    "\n",
    "    def restore_from_file(self, path: str, map_location: str = \"cpu\") -> None:\n",
    "        \"\"\"\n",
    "        The method of loading trained PyTorch model from file.\n",
    "\n",
    "        ::\n",
    "\n",
    "            clf = NeuralNetworkClassifier(\n",
    "                    Network(), nn.CrossEntropyLoss(),\n",
    "                    optim.Adam, optimizer_config, experiment\n",
    "                )\n",
    "            clf.restore_from_file('path/to/trained/weights.pth')\n",
    "\n",
    "        :param path: path to saved directory. : str\n",
    "        :param map_location: default cpu: str\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        checkpoints = torch.load(path, map_location=map_location)\n",
    "        self.restore_checkpoint(checkpoints)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9eb9e7-1e0b-4c50-b65a-6ea646e03a8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Data load-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df9eb830-f6d3-4403-acc6-4df215459899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args Values (Hardcoded)\n",
    "timestep = 1.0\n",
    "normalizer_state = None\n",
    "imputation = 'previous'\n",
    "\n",
    "train_reader = LengthOfStayReader(\n",
    "    dataset_dir=os.path.join(data_dir, 'train'),\n",
    "    listfile=os.path.join(data_dir, 'train_listfile.csv')\n",
    ")\n",
    "\n",
    "val_reader = LengthOfStayReader(\n",
    "    dataset_dir=os.path.join(data_dir, 'train'),\n",
    "    listfile=os.path.join(data_dir, 'val_listfile.csv')\n",
    ")\n",
    "\n",
    "discretizer = Discretizer(\n",
    "    timestep=timestep,\n",
    "    store_masks=True,\n",
    "    impute_strategy='previous',\n",
    "    start_time='zero'\n",
    ")\n",
    "\n",
    "discretizer_header = discretizer.transform(train_reader.read_example(0)[\"X\"])[1].split(',')\n",
    "cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
    "\n",
    "normalizer = Normalizer(fields=cont_channels)  # choose here which columns to standardize\n",
    "normalizer_state = None\n",
    "\n",
    "if normalizer_state is None:\n",
    "    normalizer_state = 'los_ts{}.input_str-previous.start_time-zero.n5e4.normalizer'.format(timestep)\n",
    "    normalizer_state = os.path.join(cur_path, normalizer_state)\n",
    "normalizer.load_params(normalizer_state)\n",
    "\n",
    "train_reader.random_shuffle()\n",
    "val_reader.random_shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84fb48e2-30f8-41a9-8f5c-a97285e83c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_chunk(data, ts, discretizer, normalizer=None):\n",
    "    data = [discretizer.transform(X, end=t)[0] for (X, t) in zip(data, ts)]\n",
    "    if normalizer is not None:\n",
    "        data = [normalizer.transform(X) for X in data]\n",
    "    return data\n",
    "\n",
    "def load_data(reader, discretizer, normalizer, max_len, chunk_size=None):\n",
    "    if chunk_size is None:\n",
    "        N = reader.get_number_of_examples()\n",
    "    else:\n",
    "        N = chunk_size\n",
    "        \n",
    "    ret = common_utils.read_chunk(reader, N)\n",
    "    data = ret[\"X\"]\n",
    "    ts = ret[\"t\"]\n",
    "    ys = ret[\"y\"]\n",
    "    names = ret[\"name\"]\n",
    "    \n",
    "    data = preprocess_chunk(data, ts, discretizer, normalizer)\n",
    "    \n",
    "    # Pad sequences so they all have the same length\n",
    "    in_feat = data[0].shape[1]\n",
    "    data_padded = np.zeros((len(data), max_len, in_feat), dtype=np.float32)\n",
    "    mask = np.zeros((len(data), max_len), dtype=np.bool_)\n",
    "    for i, x in enumerate(data):\n",
    "        data_padded[i, :x.shape[0], :] = x[-max_len:].astype(np.float32)\n",
    "        mask[i, x.shape[0]:] = True\n",
    "    \n",
    "    ys_binned = [metrics.get_bin_custom(x, 10) for x in ys]\n",
    "    \n",
    "    X = np.array(data_padded)\n",
    "    y = np.array(ys)\n",
    "    y_binned = np.array(ys_binned)\n",
    "    \n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "    y_binned = torch.tensor(y_binned, dtype=torch.long)\n",
    "    mask = torch.tensor(mask, dtype=torch.bool)\n",
    "    \n",
    "    return (X, y, y_binned, mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dfdb481-6e13-446a-bab4-d7760e423ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_reader = LengthOfStayReader(\n",
    "    dataset_dir=os.path.join(data_dir, 'test'),\n",
    "    listfile=os.path.join(data_dir, 'test_listfile.csv')\n",
    ")\n",
    "\n",
    "test_x, test_y, test_y_binned, test_mask = load_data(test_reader, discretizer, normalizer, 300, 25000)\n",
    "\n",
    "test_ds = TensorDataset(test_x, test_y, test_y_binned, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e7ea516-9e38-4fba-9de2-3b801398922d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(test_x, f\"{data_dir}/length_of_stay_test_x.pt\")\n",
    "torch.save(test_y, f\"{data_dir}/length_of_stay_test_y.pt\")\n",
    "torch.save(test_y_binned, f\"{data_dir}/length_of_stay_test_y_binned.pt\")\n",
    "torch.save(test_mask, f\"{data_dir}/length_of_stay_test_mask.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98684ef7-b774-4541-9a7d-dc5c962240cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Train SAnD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5535f4a-8fce-4764-bd4f-2fec63272f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmillikanevan\u001b[0m (\u001b[33mmillikanevan-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/mimic3-sand/wandb/run-20251117_132430-ard0lgxf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/millikanevan-personal/sand-mimic3/runs/ard0lgxf' target=\"_blank\">helpful-snowflake-72</a></strong> to <a href='https://wandb.ai/millikanevan-personal/sand-mimic3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/millikanevan-personal/sand-mimic3' target=\"_blank\">https://wandb.ai/millikanevan-personal/sand-mimic3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/millikanevan-personal/sand-mimic3/runs/ard0lgxf' target=\"_blank\">https://wandb.ai/millikanevan-personal/sand-mimic3/runs/ard0lgxf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sand = SAnD(\n",
    "    input_features = in_feature, \n",
    "    seq_len = seq_len, \n",
    "    n_heads = n_heads, \n",
    "    factor = factor,        \n",
    "    n_class = num_class,\n",
    "    n_layers = num_layers, \n",
    "    d_model = d_model, \n",
    "    lookback = lookback, \n",
    "    dropout_rate = dropout_rate\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "model = NeuralNetworkClassifier(\n",
    "    sand,\n",
    "    nn.CrossEntropyLoss(),\n",
    "    optim.Adam,\n",
    "    optimizer_config=optimizer_config,\n",
    "    experiment=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f7b57d-f9e7-4f43-bef2-0a38beb98a07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [08:08<00:00, 40.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [07:48<00:00, 42.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [07:49<00:00, 42.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [08:27<00:00, 39.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [07:51<00:00, 42.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [07:57<00:00, 41.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [07:54<00:00, 42.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [07:52<00:00, 42.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [07:52<00:00, 42.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [07:52<00:00, 42.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [07:39<00:00, 43.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [07:47<00:00, 42.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [07:45<00:00, 42.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [07:39<00:00, 43.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [07:43<00:00, 43.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [07:50<00:00, 42.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [07:48<00:00, 42.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [07:51<00:00, 42.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [07:51<00:00, 42.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002: 100%|██████████| 20000/20000 [07:39<00:00, 43.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mTraining\u001b[0m - Epochs: 002/002:  69%|██████▉   | 13824/20000 [05:03<02:11, 46.92it/s]"
     ]
    }
   ],
   "source": [
    "for i in range(num_chunks):\n",
    "    paths = [\n",
    "        f\"{data_dir}/length_of_stay_train_x_chunk{i}.pt\",\n",
    "        f\"{data_dir}/length_of_stay_train_y_chunk{i}.pt\",\n",
    "        f\"{data_dir}/length_of_stay_train_y_binned_chunk{i}.pt\",\n",
    "        f\"{data_dir}/length_of_stay_train_mask_chunk{i}.pt\",\n",
    "        f\"{data_dir}/length_of_stay_val_x_chunk{i}.pt\",\n",
    "        f\"{data_dir}/length_of_stay_val_y_chunk{i}.pt\",\n",
    "        f\"{data_dir}/length_of_stay_val_y_binned_chunk{i}.pt\",\n",
    "        f\"{data_dir}/length_of_stay_val_mask_chunk{i}.pt\"\n",
    "    ]\n",
    "    \n",
    "    if all(os.path.exists(p) for p in paths):\n",
    "        print(\"Exist\")\n",
    "        train_x = torch.load(f\"{data_dir}/length_of_stay_train_x_chunk{i}.pt\", mmap=True)\n",
    "        train_y = torch.load(f\"{data_dir}/length_of_stay_train_y_chunk{i}.pt\", mmap=True)\n",
    "        train_y_binned = torch.load(f\"{data_dir}/length_of_stay_train_y_binned_chunk{i}.pt\", mmap=True)\n",
    "        train_mask = torch.load(f\"{data_dir}/length_of_stay_train_mask_chunk{i}.pt\", mmap=True)\n",
    "        val_x   = torch.load(f\"{data_dir}/length_of_stay_val_x_chunk{i}.pt\", mmap=True)\n",
    "        val_y   = torch.load(f\"{data_dir}/length_of_stay_val_y_chunk{i}.pt\", mmap=True)\n",
    "        val_y_binned   = torch.load(f\"{data_dir}/length_of_stay_val_y_binned_chunk{i}.pt\", mmap=True)\n",
    "        val_mask   = torch.load(f\"{data_dir}/length_of_stay_val_mask_chunk{i}.pt\", mmap=True)\n",
    "    else:\n",
    "        train_x, train_y, train_y_binned, train_mask = load_data(train_reader, discretizer, normalizer, 300, 20000)\n",
    "        val_x, val_y, val_y_binned, val_mask = load_data(val_reader, discretizer, normalizer, 300, 5000)\n",
    "        torch.save(train_x, f\"{data_dir}/length_of_stay_train_x_chunk{i}.pt\")\n",
    "        torch.save(train_y, f\"{data_dir}/length_of_stay_train_y_chunk{i}.pt\")\n",
    "        torch.save(train_y_binned, f\"{data_dir}/length_of_stay_train_y_binned_chunk{i}.pt\")\n",
    "        torch.save(train_mask, f\"{data_dir}/length_of_stay_train_mask_chunk{i}.pt\")\n",
    "        torch.save(val_x,   f\"{data_dir}/length_of_stay_val_x_chunk{i}.pt\")\n",
    "        torch.save(val_y,   f\"{data_dir}/length_of_stay_val_y_chunk{i}.pt\")\n",
    "        torch.save(val_y_binned,   f\"{data_dir}/length_of_stay_val_y_binned_chunk{i}.pt\")\n",
    "        torch.save(val_mask,   f\"{data_dir}/length_of_stay_val_mask_chunk{i}.pt\")\n",
    "    \n",
    "    train_ds = TensorDataset(train_x, train_y, train_y_binned, train_mask)\n",
    "    val_ds = TensorDataset(val_x, val_y, val_y_binned, val_mask)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model.fit(\n",
    "        {\n",
    "            \"train\": train_loader,\n",
    "            \"val\": val_loader\n",
    "        },\n",
    "        epochs=num_epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b22105-8821-47f6-ac07-c4b475941a53",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Test SAnD model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddcbd4a2-a613-4dd2-a42f-2012e723f0b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "test_result = evaluate_model(model.model, test_loader, model.criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c8756ad-c9a0-43f6-847c-c850c99320e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.run.log({\n",
    "    'test_kappa' : test_result['kappa'],\n",
    "    \"test_MSE\" : test_result['MSE'],\n",
    "    'test_MAPE' : test_result['MAPE']\n",
    "})\n",
    "# model.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d7866-0958-4c0c-9104-617346e00e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_to_file(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29f7c6e-e3b7-40f6-a23a-b9ec108271d2",
   "metadata": {},
   "source": [
    "### 6. Compare with paper baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d30b6a-6b14-4d9a-b7a5-f527ac4a7dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project='sand-mimic3',\n",
    "    config={\n",
    "        \"task\" : \"length_of_stay\",\n",
    "    },\n",
    "    name='length-of-stay-baseline'\n",
    ")\n",
    "run.log({\n",
    "    'test_kappa' : 0.429,\n",
    "    \"test_MSE\" : 40373,\n",
    "    'test_MAPE' : 167.3\n",
    "})\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d9d018-27ec-45ea-a3b9-30794c635dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
